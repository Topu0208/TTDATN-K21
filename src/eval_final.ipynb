{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from entity_retrieval import surface_index_memory\n",
    "from itertools import product\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from tqdm import tqdm\n",
    "import ujson\n",
    "import re\n",
    "import requests\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nor_to_sexpr import convert_s_expression_to_sparql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\TOPU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:entity_retrieval.surface_index_memory:Loading entity vocabulary from disk.\n",
      "INFO:entity_retrieval.surface_index_memory:Loading surfaces from disk.\n",
      "INFO:entity_retrieval.surface_index_memory:Done initializing surface index.\n"
     ]
    }
   ],
   "source": [
    "surface_index = surface_index_memory.EntitySurfaceIndexMemory(\n",
    "    \"vi_entity_list_file_wikidata_complete_all_mention\", \"vi_entity_surface_map_file_wikidata_complete_all_mention\",\"vi_wiki_complete_all_mention\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_expression(expr):\n",
    "    \"\"\"Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa bi·ªÉu th·ª©c b·∫±ng c√°ch ƒë·∫øm s·ªë ngo·∫∑c m·ªü v√† ƒë√≥ng.\"\"\"\n",
    "    count = 0\n",
    "    for char in expr:\n",
    "        if char == '(':\n",
    "            count += 1\n",
    "        elif char == ')':\n",
    "            count -= 1\n",
    "        if count < 0:\n",
    "            return False  # G·∫∑p ngo·∫∑c ƒë√≥ng tr∆∞·ªõc ngo·∫∑c m·ªü\n",
    "    return count == 0\n",
    "\n",
    "def fix_unbalanced_parentheses(expr):\n",
    "    \"\"\"Lo·∫°i b·ªè ngo·∫∑c ƒë√≥ng d∆∞ n·∫øu c√≥.\"\"\"\n",
    "    while not is_valid_expression(expr) and expr.endswith(')'):\n",
    "        expr = expr[:-1]\n",
    "    return expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TOPU\\anaconda3\\envs\\py38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load model & tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\")\n",
    "model = AutoModel.from_pretrained(\"VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\")\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Tr√≠ch xu·∫•t vector embedding t·ª´ m√¥ h√¨nh\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Mean pooling\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "def find_entity_or_relation(label, label_map, facc1_index, top_k=50, similarity_threshold=0.2):\n",
    "    \"\"\"\n",
    "    T√¨m th·ª±c th·ªÉ ho·∫∑c quan h·ªá t·ª´ gold maps, SimCSE ho·∫∑c FACC1.\n",
    "    \"\"\"\n",
    "    label_lower = label.lower()\n",
    "\n",
    "    # N·∫øu c√≥ trong label_map, tr·∫£ v·ªÅ ngay\n",
    "    if label_map and label_lower in label_map:\n",
    "        return label_map[label_lower]\n",
    "\n",
    "    # L·∫•y embedding cho label\n",
    "    label_embedding = get_embedding(label_lower).reshape(1, -1)\n",
    "\n",
    "    if label_map:\n",
    "        label_keys = list(label_map.keys())\n",
    "        label_embeddings = np.array([get_embedding(k) for k in label_keys]).squeeze(1)\n",
    "\n",
    "        # ƒê·∫£m b·∫£o ƒë√∫ng shape\n",
    "        if len(label_embeddings.shape) == 1:\n",
    "            label_embeddings = label_embeddings.reshape(1, -1)\n",
    "\n",
    "        # T√≠nh cosine similarity\n",
    "        similarities = cosine_similarity(label_embedding, label_embeddings).flatten()\n",
    "\n",
    "        # Ch·ªçn th·ª±c th·ªÉ g·∫ßn nh·∫•t\n",
    "        merged_list = list(zip(label_keys, similarities))\n",
    "        sorted_list = sorted(merged_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if sorted_list and sorted_list[0][1] > similarity_threshold:\n",
    "            return label_map[sorted_list[0][0]]\n",
    "\n",
    "    # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ trong KB (FACC1)\n",
    "    facc1_cand_entities = facc1_index.get_indexrange_entity_el_pro_one_mention(label_lower, top_k=top_k)\n",
    "    if facc1_cand_entities:\n",
    "        best_match = max(facc1_cand_entities.items(), key=lambda x: x[1])\n",
    "        return best_match[0]\n",
    "\n",
    "    return label  # Tr·∫£ v·ªÅ label n·∫øu kh√¥ng t√¨m th·∫•y\n",
    "\n",
    "def parse_nsexpr(expr):\n",
    "    \"\"\"\n",
    "    Chuy·ªÉn chu·ªói bi·ªÉu th·ª©c th√†nh c√¢y c·∫•u tr√∫c d·∫°ng nested list.\n",
    "    H√†m n√†y d√πng duy·ªát k√Ω t·ª±, khi g·∫∑p '(' s·∫Ω t√¨m ph·∫ßn con cho ƒë·∫øn khi kh·ªõp v·ªõi ')',\n",
    "    v√† gi·ªØ nguy√™n n·ªôi dung trong ngo·∫∑c vu√¥ng.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(expr):\n",
    "        if expr[i].isspace():\n",
    "            i += 1\n",
    "        elif expr[i] == '(':\n",
    "            # T√¨m ph·∫ßn con c·ªßa bi·ªÉu th·ª©c trong ngo·∫∑c ƒë∆°n\n",
    "            count = 1\n",
    "            j = i + 1\n",
    "            while j < len(expr) and count > 0:\n",
    "                if expr[j] == '(':\n",
    "                    count += 1\n",
    "                elif expr[j] == ')':\n",
    "                    count -= 1\n",
    "                j += 1\n",
    "            # ƒê·ªá quy ph√¢n t√≠ch ph·∫ßn con (lo·∫°i b·ªè ngo·∫∑c bao ngo√†i)\n",
    "            subtree = parse_nsexpr(expr[i+1:j-1])\n",
    "            tokens.append(subtree)\n",
    "            i = j\n",
    "        elif expr[i] == '[':\n",
    "            # Gi·ªØ nguy√™n n·ªôi dung trong ngo·∫∑c vu√¥ng\n",
    "            j = expr.find(']', i)\n",
    "            if j == -1:\n",
    "                return \"\"\n",
    "                raise ValueError(\"Kh√¥ng t√¨m th·∫•y d·∫•u ']' k·∫øt th√∫c.\")\n",
    "                \n",
    "            token = expr[i:j+1].strip()\n",
    "            tokens.append(token)\n",
    "            i = j + 1\n",
    "        else:\n",
    "            # ƒê·ªçc m·ªôt token cho ƒë·∫øn khi g·∫∑p kho·∫£ng tr·∫Øng ho·∫∑c ngo·∫∑c\n",
    "            j = i\n",
    "            while j < len(expr) and (not expr[j].isspace()) and expr[j] not in ['(', ')']:\n",
    "                j += 1\n",
    "            tokens.append(expr[i:j])\n",
    "            i = j\n",
    "    return tokens\n",
    "\n",
    "def collect_labels(tree):\n",
    "    \"\"\"\n",
    "    Duy·ªát c√¢y c·∫•u tr√∫c (nested list) ƒë·ªÉ thu th·∫≠p c√°c nh√£n c·ªßa quan h·ªá v√† th·ª±c th·ªÉ.\n",
    "    Gi·∫£ s·ª≠:\n",
    "      - Bi·ªÉu th·ª©c JOIN c√≥ d·∫°ng: [\"JOIN\", relation_part, entity_part]\n",
    "      - Ph·∫ßn relation_part: n·∫øu l√† list v√† b·∫Øt ƒë·∫ßu b·∫±ng \"R\", th√¨ ph·∫ßn th·ª© hai ch·ª©a nh√£n quan h·ªá (d·∫°ng \"[ label ]\"). \n",
    "        N·∫øu l√† chu·ªói d·∫°ng \"[ label ]\" th√¨ ƒë√≥ c≈©ng l√† nh√£n quan h·ªá.\n",
    "      - Ph·∫ßn entity_part: n·∫øu l√† chu·ªói d·∫°ng \"[ label ]\" th√¨ ƒë√≥ l√† nh√£n th·ª±c th·ªÉ, n·∫øu l√† list th√¨ x·ª≠ l√Ω ƒë·ªá quy.\n",
    "      - Bi·ªÉu th·ª©c AND s·∫Ω c√≥ nhi·ªÅu bi·ªÉu th·ª©c con.\n",
    "    \"\"\"\n",
    "    relations = []\n",
    "    entities = []\n",
    "    \n",
    "    if isinstance(tree, list) and tree:\n",
    "        # N·∫øu token ƒë·∫ßu ti√™n l√† JOIN ho·∫∑c AND\n",
    "        op = tree[0]\n",
    "        if isinstance(op, str):\n",
    "            op_upper = op.upper()\n",
    "        else:\n",
    "            op_upper = \"\"\n",
    "        \n",
    "        if op_upper == \"JOIN\":\n",
    "            # X·ª≠ l√Ω ph·∫ßn quan h·ªá\n",
    "            if len(tree) >= 2:\n",
    "                rel_part = tree[1]\n",
    "                # N·∫øu l√† list d·∫°ng [ \"R\", \"[ label ]\" ]\n",
    "                if isinstance(rel_part, list) and len(rel_part) >= 2 and isinstance(rel_part[0], str) and rel_part[0].upper() == \"R\":\n",
    "                    token = rel_part[1]\n",
    "                    if isinstance(token, str) and token.startswith('[') and token.endswith(']'):\n",
    "                        rel_label = token[1:-1].strip()\n",
    "                        relations.append(rel_label)\n",
    "                # N·∫øu l√† chu·ªói d·∫°ng \"[ label ]\"\n",
    "                elif isinstance(rel_part, str) and rel_part.startswith('[') and rel_part.endswith(']'):\n",
    "                    rel_label = rel_part[1:-1].strip()\n",
    "                    relations.append(rel_label)\n",
    "                else:\n",
    "                    # N·∫øu kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng, duy·ªát ƒë·ªá quy\n",
    "                    sub_rel, sub_ent = collect_labels(rel_part)\n",
    "                    relations.extend(sub_rel)\n",
    "                    entities.extend(sub_ent)\n",
    "            # X·ª≠ l√Ω ph·∫ßn th·ª±c th·ªÉ\n",
    "            if len(tree) >= 3:\n",
    "                ent_part = tree[2]\n",
    "                if isinstance(ent_part, list):\n",
    "                    sub_rel, sub_ent = collect_labels(ent_part)\n",
    "                    relations.extend(sub_rel)\n",
    "                    entities.extend(sub_ent)\n",
    "                elif isinstance(ent_part, str) and ent_part.startswith('[') and ent_part.endswith(']'):\n",
    "                    ent_label = ent_part[1:-1].strip()\n",
    "                    entities.append(ent_label)\n",
    "        elif op_upper == \"AND\":\n",
    "            # V·ªõi AND, duy·ªát t·∫•t c·∫£ c√°c ph·∫ßn con\n",
    "            for sub in tree[1:]:\n",
    "                sub_rel, sub_ent = collect_labels(sub)\n",
    "                relations.extend(sub_rel)\n",
    "                entities.extend(sub_ent)\n",
    "        else:\n",
    "            # N·∫øu kh√¥ng ph·∫£i JOIN hay AND, duy·ªát t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ n·∫øu ch√∫ng l√† list\n",
    "            for elem in tree:\n",
    "                if isinstance(elem, list):\n",
    "                    sub_rel, sub_ent = collect_labels(elem)\n",
    "                    relations.extend(sub_rel)\n",
    "                    entities.extend(sub_ent)\n",
    "    return relations, entities\n",
    "\n",
    "\n",
    "\n",
    "def extract_entities_and_relations(normed_expr):\n",
    "\n",
    "    if not normed_expr or len(normed_expr) == 0:  # Ki·ªÉm tra n·∫øu normed_expr r·ªóng\n",
    "        return [], []\n",
    "    \n",
    "    if normed_expr[0] != \"(\":\n",
    "        return [], []\n",
    "    \n",
    "    tree = parse_nsexpr(normed_expr)\n",
    "    if tree is None:\n",
    "        return [], []  # Tr·∫£ v·ªÅ danh s√°ch r·ªóng n·∫øu parse th·∫•t b·∫°i\n",
    "    \n",
    "    return collect_labels(tree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity(label, label_map, facc1_index, top_k=20, similarity_threshold=0.2):\n",
    "    \"\"\"\n",
    "    T√¨m th·ª±c th·ªÉ ho·∫∑c quan h·ªá t·ª´ gold maps, SimCSE ho·∫∑c FACC1.\n",
    "    \"\"\"\n",
    "    label_lower = label.lower()\n",
    "\n",
    "    # N·∫øu c√≥ trong label_map, tr·∫£ v·ªÅ ngay\n",
    "    if label_map and label_lower in label_map:\n",
    "        return label_map[label_lower]\n",
    "\n",
    "    # L·∫•y embedding cho label\n",
    "    label_embedding = get_embedding(label_lower).reshape(1, -1)\n",
    "\n",
    "    if label_map:\n",
    "        label_keys = list(label_map.keys())\n",
    "        label_embeddings = np.array([get_embedding(k) for k in label_keys]).squeeze(1)\n",
    "        label_embedding = normalize(label_embedding, axis=1)\n",
    "        label_embeddings = normalize(label_embeddings, axis=1)\n",
    "        # T√≠nh cosine similarity\n",
    "        similarities = cosine_similarity(label_embedding, label_embeddings).flatten()\n",
    "        # Ch·ªçn th·ª±c th·ªÉ g·∫ßn nh·∫•t\n",
    "        merged_list = list(zip(label_keys, similarities))\n",
    "        sorted_list = sorted(merged_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if sorted_list and sorted_list[0][1] > similarity_threshold:\n",
    "            return label_map[sorted_list[0][0]]\n",
    "\n",
    "    # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ trong KB (FACC1)\n",
    "    facc1_cand_entities = facc1_index.get_indexrange_entity_el_pro_one_mention(label_lower, top_k=top_k)\n",
    "    if facc1_cand_entities:\n",
    "        temp = []\n",
    "        for key in list(facc1_cand_entities.keys())[1:]:\n",
    "            if facc1_cand_entities[key] >= 0.001:\n",
    "                temp.append(key)\n",
    "        if len(temp) > 0:\n",
    "            label = [list(facc1_cand_entities.keys())[0]]+temp\n",
    "        else:\n",
    "            label = list(facc1_cand_entities.keys())[0]\n",
    "\n",
    "    return label  # Tr·∫£ v·ªÅ label n·∫øu kh√¥ng t√¨m th·∫•y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relation(label, label_map, relation_kb_map, facc1_index, top_k=20, similarity_threshold=0.2):\n",
    "    \"\"\"\n",
    "    T√¨m quan h·ªá t·ª´ gold maps, SimCSE ho·∫∑c FACC1.\n",
    "    \"\"\"\n",
    "    label_lower = label.lower()\n",
    "\n",
    "    # üîπ B∆Ø·ªöC 1: Ki·ªÉm tra n·∫øu ƒë√£ c√≥ s·∫µn trong `label_map`\n",
    "    if label_map and label_lower in label_map:\n",
    "        return label_map[label_lower]\n",
    "\n",
    "    # üîπ B∆Ø·ªöC 2: Ki·ªÉm tra n·∫øu c√≥ s·∫µn trong `relation_kb_map`\n",
    "    if relation_kb_map and label_lower in relation_kb_map:\n",
    "        return relation_kb_map[label_lower]  # ‚è© Tr·∫£ v·ªÅ ngay n·∫øu c√≥ s·∫µn\n",
    "\n",
    "    # üîπ B∆Ø·ªöC 3: T√≠nh to√°n embedding v√† t√¨m quan h·ªá g·∫ßn nh·∫•t b·∫±ng cosine similarity\n",
    "    label_embedding = get_embedding(label_lower).reshape(1, -1)\n",
    "\n",
    "    if label_map:\n",
    "        label_keys = list(label_map.keys())\n",
    "        label_embeddings = np.array([get_embedding(k) for k in label_keys]).squeeze(1)\n",
    "        label_embedding = normalize(label_embedding, axis=1)\n",
    "        label_embeddings = normalize(label_embeddings, axis=1)\n",
    "\n",
    "        similarities = cosine_similarity(label_embedding, label_embeddings).flatten()\n",
    "        sorted_list = sorted(zip(label_keys, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if sorted_list and sorted_list[0][1] > similarity_threshold:\n",
    "            return label_map[sorted_list[0][0]]\n",
    "\n",
    "    if relation_kb_map:\n",
    "        label_keys = list(relation_kb_map.keys())\n",
    "        label_embeddings = np.array([get_embedding(k) for k in label_keys]).squeeze(1)\n",
    "        label_embedding = normalize(label_embedding, axis=1)\n",
    "        label_embeddings = normalize(label_embeddings, axis=1)\n",
    "\n",
    "        similarities = cosine_similarity(label_embedding, label_embeddings).flatten()\n",
    "        sorted_list = sorted(zip(label_keys, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if sorted_list and sorted_list[0][1] > similarity_threshold:\n",
    "            return relation_kb_map[sorted_list[0][0]]\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SExpressionParser:\n",
    "    def __init__(self):\n",
    "        self.var_counter = 1  # ƒê·∫øm s·ªë bi·∫øn trung gian (?X1, ?X2, ...)\n",
    "\n",
    "    def get_new_var(self):\n",
    "        \"\"\"T·∫°o bi·∫øn trung gian m·ªõi.\"\"\"\n",
    "        var_name = f\"?X{self.var_counter}\"\n",
    "        self.var_counter += 1\n",
    "        return var_name\n",
    "\n",
    "    def parse_s_expr(self, s_expr):\n",
    "        \"\"\"Chuy·ªÉn ƒë·ªïi S-Expression th√†nh danh s√°ch l·ªìng nhau.\"\"\"\n",
    "        s_expr = re.sub(r'\\(', ' ( ', s_expr)\n",
    "        s_expr = re.sub(r'\\)', ' ) ', s_expr)\n",
    "        tokens = s_expr.split()\n",
    "        return self.build_tree(tokens)\n",
    "\n",
    "    def build_tree(self, tokens):\n",
    "        \"\"\"Chuy·ªÉn ƒë·ªïi danh s√°ch token th√†nh c√¢y l·ªìng nhau.\"\"\"\n",
    "        if not tokens:\n",
    "            return None\n",
    "        token = tokens.pop(0)\n",
    "        if token == \"(\":\n",
    "            sub_expr = []\n",
    "            while tokens[0] != \")\":\n",
    "                sub_expr.append(self.build_tree(tokens))\n",
    "            tokens.pop(0)  # B·ªè d·∫•u \")\"\n",
    "            return sub_expr\n",
    "        elif token == \")\":\n",
    "            raise ValueError(\"Unexpected ')'\")\n",
    "        else:\n",
    "            return token\n",
    "\n",
    "    def process_join(self, expr, target_var):\n",
    "        \"\"\"\n",
    "        X·ª≠ l√Ω JOIN, t·∫°o triple SPARQL.\n",
    "        \"\"\"\n",
    "        triples = []\n",
    "        if not isinstance(expr, list):\n",
    "            return expr, triples\n",
    "\n",
    "        if expr[0] == \"AND\":\n",
    "            # X·ª≠ l√Ω t·ª´ng JOIN trong AND ri√™ng l·∫ª\n",
    "            for sub_expr in expr[1:]:\n",
    "                _, sub_triples = self.process_join(sub_expr, target_var)\n",
    "                triples.extend(sub_triples)\n",
    "            return target_var, triples\n",
    "\n",
    "        if expr[0] == \"JOIN\":\n",
    "            right_expr = expr[2]\n",
    "            right_triples = []\n",
    "            if isinstance(right_expr, list) and right_expr[0] == \"JOIN\":\n",
    "                right_var, right_triples = self.process_join(right_expr, self.get_new_var())\n",
    "            else: \n",
    "                right_var = right_expr\n",
    "            \n",
    "            # X·ª≠ l√Ω nh√°nh tr√°i\n",
    "            left_expr = expr[1]\n",
    "            if isinstance(left_expr, list) and left_expr[0] == \"R\":\n",
    "                rel = left_expr[1]\n",
    "                right = right_var\n",
    "                left = target_var\n",
    "                if right[0] != '?':\n",
    "                    right = \"wd:\" + right\n",
    "                if left[0] !='?':\n",
    "                    left = \"wd:\" + left   \n",
    "                triples.append([right, f\"wdt:{rel}\", left])\n",
    "            else:\n",
    "                right = right_var\n",
    "                left = target_var\n",
    "                if right[0] != '?':\n",
    "                    right = \"wd:\" + right\n",
    "                if left[0] !='?':\n",
    "                    left = \"wd:\" + left   \n",
    "                triples.append([left, f\"wdt:{left_expr}\", right])\n",
    "\n",
    "            # Th√™m c√°c triples t·ª´ nh√°nh ph·∫£i tr∆∞·ªõc khi th√™m triple ch√≠nh\n",
    "            triples = right_triples + triples\n",
    "            return target_var, triples\n",
    "\n",
    "        return expr, triples\n",
    "\n",
    "    def s_expr_to_sparql(self, s_expr):\n",
    "        \"\"\"Chuy·ªÉn ƒë·ªïi t·ª´ S-Expression sang SPARQL.\"\"\"\n",
    "        if s_expr.count(\"(\") != s_expr.count(\")\"):\n",
    "            return None\n",
    "        if s_expr.count(\"[\") != s_expr.count(\"]\"):\n",
    "            return None\n",
    "        parsed_expr = self.parse_s_expr(s_expr)\n",
    "        target_var = \"?answer\"\n",
    "        final_var, triples = self.process_join(parsed_expr, target_var)\n",
    "\n",
    "        sparql_body = \"\\n  \".join([\" \".join(t) + \" .\" for t in triples])\n",
    "        sparql_query = f\"\"\"PREFIX wd: <http://www.wikidata.org/entity/> \n",
    "PREFIX wdt: <http://www.wikidata.org/prop/direct/> \n",
    "SELECT DISTINCT {target_var} WHERE {{ \n",
    "  {sparql_body}\n",
    "}}\"\"\"\n",
    "        return sparql_query\n",
    "\n",
    "import requests    \n",
    "WIKIDATA_SPARQL_ENDPOINT = \"https://query.wikidata.org/sparql\"\n",
    "WIKIDATA_API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "def execute_query_with_odbc(sparql_query):\n",
    "    \"\"\"Truy v·∫•n Wikidata v√† tr·∫£ v·ªÅ danh s√°ch c√¢u tr·∫£ l·ªùi (bao g·ªìm t·∫•t c·∫£ bi·∫øn)\"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\", \"Accept\": \"application/sparql-results+json\"}\n",
    "    response = requests.get(WIKIDATA_SPARQL_ENDPOINT, params={\"query\": sparql_query, \"format\": \"json\"}, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        results = response.json().get(\"results\", {}).get(\"bindings\", [])\n",
    "        answers = []\n",
    "\n",
    "        for result in results:\n",
    "            for var in result:  # Duy·ªát qua t·∫•t c·∫£ c√°c bi·∫øn tr·∫£ v·ªÅ\n",
    "                value = result[var][\"value\"]\n",
    "                answers.append(value)  # Ch·∫•p nh·∫≠n t·∫•t c·∫£ gi√° tr·ªã, kh√¥ng ch·ªâ th·ª±c th·ªÉ Wikidata\n",
    "\n",
    "        return answers  # Tr·∫£ v·ªÅ to√†n b·ªô danh s√°ch k·∫øt qu·∫£\n",
    "\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_normed_to_s_expression(normed_expr, gold_relation_map, gold_entity_map, relation_KB_map, facc1_index):\n",
    "    \"\"\"\n",
    "    Chuy·ªÉn ƒë·ªïi t·ª´ normed_sexpression sang s_expression.\n",
    "    Sau khi tr√≠ch xu·∫•t c√°c nh√£n quan h·ªá v√† th·ª±c th·ªÉ t·ª´ normed_expr,\n",
    "    ta l·∫•y danh s√°ch c√°c m√£ ·ª©ng vi√™n cho m·ªói nh√£n v√† t·∫°o ho√°n v·ªã gi·ªØa c√°c c·∫∑p ·ª©ng vi√™n ƒë√≥.\n",
    "    K·∫øt qu·∫£ tr·∫£ v·ªÅ l√† m·ªôt danh s√°ch c√°c s_expression kh·∫£ dƒ©.\n",
    "    \"\"\"\n",
    "    normed_expr = fix_unbalanced_parentheses(normed_expr)\n",
    "    # Tr√≠ch xu·∫•t c√°c nh√£n quan h·ªá v√† th·ª±c th·ªÉ t·ª´ bi·ªÉu th·ª©c\n",
    "    relations, entities = extract_entities_and_relations(normed_expr)\n",
    "    \n",
    "    # T·∫°o mapping t·ª´ token xu·∫•t hi·ªán trong bi·ªÉu th·ª©c sang danh s√°ch c√°c ·ª©ng vi√™n m√£.\n",
    "    # V√≠ d·ª•: token_str = \"[ author ]\"\n",
    "    candidate_map = {}\n",
    "    for rel in relations:\n",
    "        token = f'[ {rel} ]'\n",
    "        candidate = find_relation(rel, gold_relation_map, relation_KB_map, facc1_index)\n",
    "        # N·∫øu candidate kh√¥ng ph·∫£i danh s√°ch, chuy·ªÉn n√≥ th√†nh danh s√°ch ƒë·ªÉ t·∫°o ho√°n v·ªã\n",
    "        if not isinstance(candidate, list):\n",
    "            candidate = [candidate]\n",
    "        candidate_map[token] = candidate\n",
    "        \n",
    "    for ent in entities:\n",
    "        token = f'[ {ent} ]'\n",
    "        candidate = find_entity(ent, gold_entity_map , facc1_index)\n",
    "        if not isinstance(candidate, list):\n",
    "            candidate = [candidate]\n",
    "        candidate_map[token] = candidate\n",
    "    \n",
    "    # N·∫øu kh√¥ng c√≥ token n√†o c·∫ßn thay th·∫ø, tr·∫£ v·ªÅ bi·ªÉu th·ª©c g·ªëc\n",
    "    if not candidate_map:\n",
    "        return [normed_expr]\n",
    "    \n",
    "    # L·∫•y danh s√°ch c√°c token v√† danh s√°ch c√°c danh s√°ch ·ª©ng vi√™n t∆∞∆°ng ·ª©ng\n",
    "    tokens = list(candidate_map.keys())\n",
    "    candidate_lists = [candidate_map[token] for token in tokens]\n",
    "    \n",
    "    # T·∫°o t·∫•t c·∫£ c√°c ho√°n v·ªã ·ª©ng vi√™n (Cartesian product)\n",
    "    all_combinations = list(product(*candidate_lists))\n",
    "    \n",
    "    s_expressions = []\n",
    "    for comb in all_combinations:\n",
    "        temp_expr = normed_expr\n",
    "        # V·ªõi m·ªói token, thay th·∫ø b·∫±ng ·ª©ng vi√™n t∆∞∆°ng ·ª©ng theo ho√°n v·ªã\n",
    "        for token, replacement in zip(tokens, comb):\n",
    "            temp_expr = temp_expr.replace(token, replacement)\n",
    "        s_expressions.append(temp_expr)\n",
    "    \n",
    "    return s_expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"ƒê·ªçc file JSONL v√† tr·∫£ v·ªÅ danh s√°ch c√°c object\"\"\"\n",
    "\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read().replace('(EXPECTED RESULT)', 'null').replace('(QUESTION)', 'null')\n",
    "\n",
    "    try:\n",
    "        data = ujson.loads(content)\n",
    "        print(\"JSON loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prf1(gold_answers, pred_answers):\n",
    "    \"\"\"T√≠nh Precision, Recall, F1-score\"\"\"\n",
    "    if len(gold_answers) == 0:\n",
    "        if len(pred_answers) == 0:\n",
    "            return [1.0, 1.0, 1.0]  # ƒê√∫ng khi kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi\n",
    "        else:\n",
    "            return [0.0, 1.0, 0.0]\n",
    "    elif len(pred_answers) == 0:\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    tp = 1e-40  # numerical trick\n",
    "\n",
    "    tp = tp + len(set(gold_answers) & set(pred_answers))\n",
    "\n",
    "    fp = len(set(pred_answers) - set(gold_answers))\n",
    "    fn = len(set(gold_answers) - set(pred_answers))\n",
    "    precision = tp / (tp + fp) \n",
    "    recall = tp / (tp + fn) \n",
    "    f1 = (2 * precision * recall) / (precision + recall) \n",
    "\n",
    "    return [precision, recall, f1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON loaded successfully!\n",
      "JSON loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªçc d·ªØ li·ªáu\n",
    "predictions = load_jsonl(\"LLMs/beam_prediction/generated_predictions_beam_Q7b.json\")\n",
    "gold_data = load_jsonl(\"Data/LC-QuAD2.0/label_map/LC-QuAD2.0_test.json\")\n",
    "gold_data = gold_data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load danh s√°ch quan h·ªá trong KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"property_list_file_wikidata_complete_all_mention\"\n",
    "\n",
    "relation_KB_map = {}\n",
    "\n",
    "# ƒê·ªçc file v√† ƒë·∫£o ng∆∞·ª£c quan h·ªá\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")  # T√°ch theo tab\n",
    "        if len(parts) == 2:\n",
    "            relation_KB_map[parts[1]] = parts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_cnt = 0\n",
    "top_hit = 0\n",
    "failed_preds = []\n",
    "final_executable_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ƒêang ƒë√°nh gi√°:   0%|          | 0/1000 [05:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lag_result:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m set_query \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_normed_to_s_expression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgold_relation_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgold_entity_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelation_KB_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfacc1_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m set_query:\n\u001b[0;32m     34\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m, in \u001b[0;36mconvert_normed_to_s_expression\u001b[1;34m(normed_expr, gold_relation_map, gold_entity_map, relation_KB_map, facc1_index)\u001b[0m\n\u001b[0;32m      8\u001b[0m normed_expr \u001b[38;5;241m=\u001b[39m fix_unbalanced_parentheses(normed_expr)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Tr√≠ch xu·∫•t c√°c nh√£n quan h·ªá v√† th·ª±c th·ªÉ t·ª´ bi·ªÉu th·ª©c\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m relations, entities \u001b[38;5;241m=\u001b[39m \u001b[43mextract_entities_and_relations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormed_expr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# T·∫°o mapping t·ª´ token xu·∫•t hi·ªán trong bi·ªÉu th·ª©c sang danh s√°ch c√°c ·ª©ng vi√™n m√£.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# V√≠ d·ª•: token_str = \"[ author ]\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m candidate_map \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[6], line 179\u001b[0m, in \u001b[0;36mextract_entities_and_relations\u001b[1;34m(normed_expr)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normed_expr[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], []\n\u001b[1;32m--> 179\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[43mparse_nsexpr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormed_expr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tree \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], []  \u001b[38;5;66;03m# Tr·∫£ v·ªÅ danh s√°ch r·ªóng n·∫øu parse th·∫•t b·∫°i\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 98\u001b[0m, in \u001b[0;36mparse_nsexpr\u001b[1;34m(expr)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# ƒê·ªçc m·ªôt token cho ƒë·∫øn khi g·∫∑p kho·∫£ng tr·∫Øng ho·∫∑c ngo·∫∑c\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     j \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m expr[j]\u001b[38;5;241m.\u001b[39misspace()) \u001b[38;5;129;01mand\u001b[39;00m expr[j] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     99\u001b[0m         j \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    100\u001b[0m     tokens\u001b[38;5;241m.\u001b[39mappend(expr[i:j])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "BATCH_SIZE = 300  # S·ªë d√≤ng m·ªói batch\n",
    "LOG_FILE = \"progress_log.txt\"\n",
    "\n",
    "results = []\n",
    "simcse_model = model  # M√¥ h√¨nh SimCSE c·ªßa b·∫°n\n",
    "facc1_index = surface_index\n",
    "parser = SExpressionParser()\n",
    "start_time = time.perf_counter()  # B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian\n",
    "\n",
    "# Ghi log\n",
    "with open(LOG_FILE, \"w\", encoding=\"utf-8\") as log_file:\n",
    "    log_file.write(\"B·∫Øt ƒë·∫ßu qu√° tr√¨nh ƒë√°nh gi√°\\n\")\n",
    "\n",
    "for i, (pred, gold) in enumerate(tqdm(zip(predictions, gold_data), total=len(predictions), desc=\"ƒêang ƒë√°nh gi√°\")):\n",
    "    try:\n",
    "        gold_entity_map = {v.lower(): k for k, v in gold['gold_entity_map'].items()}\n",
    "        gold_relation_map = {v.lower(): k for k, v in gold['gold_relation_map'].items()}\n",
    "        gold_answers = gold.get(\"answer\", [])\n",
    "        executable_index = None\n",
    "        best_f1, best_precision, best_recall = 0, 0, 0\n",
    "        kq = []\n",
    "        lag_result = False\n",
    "        denormed_pred = []\n",
    "        for rank, query in enumerate(pred['predicted_query']):\n",
    "            if lag_result:\n",
    "                break\n",
    "\n",
    "            set_query = convert_normed_to_s_expression(query, gold_relation_map, gold_entity_map, relation_KB_map , facc1_index)\n",
    "            for q in set_query:\n",
    "                query_result = []\n",
    "                if not q:\n",
    "                    continue\n",
    "                if rank == 0 and q.lower() ==gold['s_expr'].lower():\n",
    "                    ex_cnt +=1\n",
    "                sparql = convert_s_expression_to_sparql(q)\n",
    "                if sparql == \"UNKNOWN\":\n",
    "                    continue\n",
    "                denormed_pred.append(sparql)\n",
    "                query_result = execute_query_with_odbc(sparql)\n",
    "                if query_result:\n",
    "                    if rank == 0:\n",
    "                        top_hit += 1\n",
    "                    executable_index = rank\n",
    "                    precision, recall, f1 = calculate_prf1(gold_answers, query_result)\n",
    "                    if f1 > best_f1:\n",
    "                        kq = query_result\n",
    "                        best_f1, best_precision, best_recall = f1, precision, recall\n",
    "                    if precision == 1:\n",
    "                        lag_result = True\n",
    "                        break\n",
    "        if executable_index is not None:\n",
    "            final_executable_cnt+=1\n",
    "        else:\n",
    "            failed_preds.append({'qid':gold[\"question_id\"], \n",
    "                'gt_sexpr': gold['s_expr'], \n",
    "                'gt_normed_sexpr': pred['gen_label'],\n",
    "                'pred': pred, \n",
    "                'denormed_pred':denormed_pred})    \n",
    "        results.append({\n",
    "            \"qid\": gold[\"question_id\"],\n",
    "            \"answer\": gold_answers,\n",
    "            \"result\": kq,\n",
    "            \"nor_s_expr\":gold[\"nor_s_expr\"],\n",
    "            \"precision\": best_precision,\n",
    "            \"recall\": best_recall,\n",
    "            \"f1\": best_f1\n",
    "        })\n",
    "\n",
    "        # Ghi log ti·∫øn tr√¨nh x·ª≠ l√Ω\n",
    "        with open(LOG_FILE, \"a\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(f\"ƒê√£ x·ª≠ l√Ω xong d√≤ng {i + 1}/{len(predictions)}\\n\")\n",
    "\n",
    "        # Khi ƒë·ªß 100 k·∫øt qu·∫£, l∆∞u v√†o file v√† reset bi·∫øn `results`\n",
    "        if (i + 1) % BATCH_SIZE == 0 or (i + 1) == len(predictions):\n",
    "            batch_id = (i + 1) // BATCH_SIZE\n",
    "            filename = f\"LLMs/eval_result/evaluation_vinallamaQ7b_part_{batch_id}.json\"\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            print(f\"‚úÖ ƒê√£ l∆∞u {len(results)} d√≤ng v√†o {filename}\")\n",
    "            results = []  # Reset danh s√°ch k·∫øt qu·∫£\n",
    "\n",
    "    except Exception as e:\n",
    "        with open(LOG_FILE, \"a\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(f\"L·ªói t·∫°i d√≤ng {i + 1}: {str(e)}\\n\")\n",
    "        print(f\"‚ùå L·ªói t·∫°i d√≤ng {i + 1}: {e}\")\n",
    "\n",
    "# K·∫øt th√∫c ƒëo th·ªùi gian\n",
    "end_time = time.perf_counter()\n",
    "total_time = end_time - start_time\n",
    "print(f\"üéØ Qu√° tr√¨nh ƒë√°nh gi√° ho√†n t·∫•t trong {total_time:.2f} gi√¢y!\")\n",
    "\n",
    "with open(LOG_FILE, \"a\", encoding=\"utf-8\") as log_file:\n",
    "    log_file.write(f\"Qu√° tr√¨nh ƒë√°nh gi√° ho√†n t·∫•t trong {total_time:.2f} gi√¢y!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STR Match 0.34690943938667945\n",
      "TOP 1 Executable 0.45759463344513657\n",
      "Final Executable 0.6411116435074269\n"
     ]
    }
   ],
   "source": [
    "print('STR Match', ex_cnt/ len(predictions))\n",
    "print('TOP 1 Executable', top_hit/ len(predictions))\n",
    "print('Final Executable', final_executable_cnt/ len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STR_Match = ex_cnt/ len(predictions)\n",
    "TOP1_Executable = top_hit/ len(predictions)\n",
    "Final_Executable = final_executable_cnt/ len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Precision: 0.51899\n",
      "üìä Recall: 0.53568\n",
      "üìä F1-score: 0.51993\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Danh s√°ch c√°c file k·∫øt qu·∫£ ƒë√°nh gi√°\n",
    "file_list = glob.glob(\"LLMs/eval_result/evaluation_vinallamaQ7b_part_*.json\")\n",
    "\n",
    "# Bi·∫øn ƒë·ªÉ t·ªïng h·ª£p k·∫øt qu·∫£\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_f1 = 0\n",
    "total_samples = 0\n",
    "\n",
    "# ƒê·ªçc t·ª´ng file v√† t·ªïng h·ª£p d·ªØ li·ªáu\n",
    "for file in file_list:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        for entry in data:\n",
    "            total_precision += entry.get(\"precision\", 0)\n",
    "            total_recall += entry.get(\"recall\", 0)\n",
    "            total_f1 += entry.get(\"f1\", 0)\n",
    "            total_samples += 1\n",
    "\n",
    "# Tr√°nh chia cho 0\n",
    "if total_samples > 0:\n",
    "    avg_precision = total_precision / total_samples\n",
    "    avg_recall = total_recall / total_samples\n",
    "    avg_f1 = total_f1 / total_samples\n",
    "else:\n",
    "    avg_precision, avg_recall, avg_f1, hits_at_1 = 0, 0, 0, 0\n",
    "\n",
    "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
    "print(f\"üìä Precision: {avg_precision:.5f}\")\n",
    "print(f\"üìä Recall: {avg_recall:.5f}\")\n",
    "print(f\"üìä F1-score: {avg_f1:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o LLMs/eval_result/Final_evaluation_27b.json\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o d·ªØ li·ªáu k·∫øt qu·∫£ ƒë√°nh gi√°\n",
    "eval_results = {\n",
    "    \"precision\": avg_precision,\n",
    "    \"recall\": avg_recall,\n",
    "    \"f1\": avg_f1,\n",
    "    \"STR Match\": STR_Match,\n",
    "    \"Hit@1\": TOP1_Executable,\n",
    "    \"Final_Executable\": Final_Executable, \n",
    "}\n",
    "\n",
    "# L∆∞u v√†o file JSON\n",
    "output_file = \"LLMs/eval_result/Final_evaluation_Q7b.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eval_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o LLMs/eval_result/Failed_result_27b.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_file = \"LLMs/eval_result/Failed_result_Q7b.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(failed_preds, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
