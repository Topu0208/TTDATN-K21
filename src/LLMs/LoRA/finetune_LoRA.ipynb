{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":10724114,"datasetId":6648045,"databundleVersionId":11073891},{"sourceType":"datasetVersion","sourceId":10733334,"datasetId":6654879,"databundleVersionId":11084166},{"sourceType":"datasetVersion","sourceId":10784476,"datasetId":6692233,"databundleVersionId":11141096}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Kết quả: True (nếu GPU đã bật)\nprint(torch.cuda.get_device_name(0))  # Kiểm tra loại GPU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:29:53.495784Z","iopub.execute_input":"2025-02-18T14:29:53.496064Z","iopub.status.idle":"2025-02-18T14:29:57.341777Z","shell.execute_reply.started":"2025-02-18T14:29:53.496035Z","shell.execute_reply":"2025-02-18T14:29:57.341050Z"}},"outputs":[{"name":"stdout","text":"True\nTesla T4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import load_dataset\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:29:57.342540Z","iopub.execute_input":"2025-02-18T14:29:57.342846Z","iopub.status.idle":"2025-02-18T14:30:17.583367Z","shell.execute_reply.started":"2025-02-18T14:29:57.342826Z","shell.execute_reply":"2025-02-18T14:30:17.582460Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16,  # Rank của LoRA, tăng giá trị này giúp học nhiều hơn nhưng tốn VRAM\n    lora_alpha=32,  # Hệ số điều chỉnh mức độ học của LoRA\n    lora_dropout=0.05,  # Dropout giúp tránh overfitting\n    bias=\"none\",  # Không cập nhật bias để tiết kiệm tài nguyên\n    task_type=TaskType.CAUSAL_LM,  # Loại tác vụ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:30:17.584192Z","iopub.execute_input":"2025-02-18T14:30:17.584775Z","iopub.status.idle":"2025-02-18T14:30:17.588692Z","shell.execute_reply.started":"2025-02-18T14:30:17.584749Z","shell.execute_reply":"2025-02-18T14:30:17.587816Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"model_path = \"vilm/vinallama-7b\"\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.float16)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\"Mô hình đã tải thành công với LoRA!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:41:35.275775Z","iopub.execute_input":"2025-02-18T14:41:35.276094Z","iopub.status.idle":"2025-02-18T14:41:42.273289Z","shell.execute_reply.started":"2025-02-18T14:41:35.276063Z","shell.execute_reply":"2025-02-18T14:41:42.272296Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ced0d9089b1468db0a9c82fad67a331"}},"metadata":{}},{"name":"stdout","text":"Mô hình đã tải thành công với LoRA!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from peft import get_peft_model\n\nfor param in model.parameters():\n    param.requires_grad = True\n\nmodel = get_peft_model(model, lora_config)  # Kích hoạt LoRA\nmodel.train()\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:41:42.274651Z","iopub.execute_input":"2025-02-18T14:41:42.274990Z","iopub.status.idle":"2025-02-18T14:41:42.467567Z","shell.execute_reply.started":"2025-02-18T14:41:42.274957Z","shell.execute_reply":"2025-02-18T14:41:42.466484Z"}},"outputs":[{"name":"stdout","text":"trainable params: 8,388,608 || all params: 6,863,974,400 || trainable%: 0.1222\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import json\nfrom datasets import Dataset\n\n# Load dataset từ Kaggle input\ndata_path = \"/kaggle/input/train-data/examples.json\"\n\nwith open(data_path, \"r\", encoding=\"utf-8\") as f:\n    dataset = json.load(f)\n\n# Kiểm tra dữ liệu đầu vào\nassert isinstance(dataset, list), \"Dataset JSON phải là danh sách các mục\"\n\n# Chuyển đổi dữ liệu sang dạng văn bản huấn luyện\ntrain_texts = [\n    f\"Instruction:\\n{item['instruction']}\\n\\nInput:\\n{item['input']}\\n\\nOutput:\\n{item['output']}\"\n    for item in dataset if \"instruction\" in item and \"input\" in item and \"output\" in item\n]\n\n# Tạo dataset từ Hugging Face Datasets\ntrain_dataset = Dataset.from_dict({\"text\": train_texts})\n\n# Cấu hình tokenizer\ntokenizer.pad_token = tokenizer.eos_token\n\n# Tokenize dữ liệu\ndef tokenize_function(example):\n    tokenized = tokenizer(\n        example[\"text\"],  \n        padding=\"max_length\",  # Có thể đổi thành \"longest\" nếu muốn linh hoạt hơn\n        truncation=True,\n        max_length=512,\n        return_tensors=\"pt\",\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # ✅ Thêm labels\n    return tokenized\n\n# Tokenize dataset\ntokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n\n# Kiểm tra kết quả\nprint(tokenized_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:41:42.469288Z","iopub.execute_input":"2025-02-18T14:41:42.469495Z","iopub.status.idle":"2025-02-18T14:41:44.538673Z","shell.execute_reply.started":"2025-02-18T14:41:42.469477Z","shell.execute_reply":"2025-02-18T14:41:44.537982Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba695e88fe3b436ba87b90fd6db62afd"}},"metadata":{}},{"name":"stdout","text":"{'input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2799, 4080, 29901, 13, 29911, 32168, 33002, 32557, 4522, 936, 3812, 32073, 32665, 32219, 27773, 32828, 32568, 32034, 32889, 32723, 32030, 3060, 29889, 29871, 13, 13, 13, 4290, 29901, 13, 16492, 29901, 426, 1281, 32009, 32641, 22392, 7712, 2681, 306, 18916, 7468, 29973, 500, 13, 13, 6466, 29901, 13, 29898, 8780, 313, 390, 518, 10329, 4514, 1723, 313, 8780, 313, 390, 518, 32040, 32797, 36396, 4514, 1723, 518, 22392, 7712, 2681, 306, 4514, 1723, 1723], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2799, 4080, 29901, 13, 29911, 32168, 33002, 32557, 4522, 936, 3812, 32073, 32665, 32219, 27773, 32828, 32568, 32034, 32889, 32723, 32030, 3060, 29889, 29871, 13, 13, 13, 4290, 29901, 13, 16492, 29901, 426, 1281, 32009, 32641, 22392, 7712, 2681, 306, 18916, 7468, 29973, 500, 13, 13, 6466, 29901, 13, 29898, 8780, 313, 390, 518, 10329, 4514, 1723, 313, 8780, 313, 390, 518, 32040, 32797, 36396, 4514, 1723, 518, 22392, 7712, 2681, 306, 4514, 1723, 1723]}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nfrom transformers import DataCollatorWithPadding\n\nMICRO_BATCH_SIZE = 1  # ✅ Giảm xuống 1 để tiết kiệm VRAM\nGRADIENT_ACCUMULATION_STEPS = 16  # ✅ Tổng batch size = 1 * 16 = 16\nTRAIN_STEPS = 600  # ✅ Train lâu hơn\nLEARNING_RATE = 1e-4  # ✅ Giảm để tránh overfitting\n\n# Khởi tạo collator với tokenizer đã load\ndata_collator = DataCollatorWithPadding(\n    tokenizer=tokenizer, \n    padding=True\n)\n\n# Cấu hình TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"./output\",\n    per_device_train_batch_size=MICRO_BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    warmup_ratio=0.03,\n    max_steps=TRAIN_STEPS,\n    learning_rate=LEARNING_RATE,\n    logging_steps=50,\n    optim='adamw_torch',\n    save_strategy=\"steps\",\n    save_steps=500,  # ✅ Giảm số lần lưu checkpoint để tránh lãng phí bộ nhớ\n    report_to=\"tensorboard\",\n    save_total_limit=1,\n    fp16=True,  # ✅ Bật FP16 để tiết kiệm VRAM\n    remove_unused_columns=False,  # ✅ Fix lỗi Trainer không nhận dữ liệu\n    gradient_checkpointing=False,  # ✅ Giảm bộ nhớ tiêu thụ khi huấn luyện\n)\n\n# Khởi tạo Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,  # ✅ Đảm bảo padding đúng\n)\n\n# Bắt đầu huấn luyện\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:04:47.164842Z","iopub.execute_input":"2025-02-18T15:04:47.165220Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='270' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [270/600 1:09:58 < 1:26:09, 0.06 it/s, Epoch 0.65/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>2.311100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.188500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.131700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.024300</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.977900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"def generate_query(question, history=[]):\n    input_text = f\"Instruction:\\nTạo truy vấn dạng logic để lấy thông tin tương ứng với câu hỏi đã cho.\\n\\n Input:\\nQuestion: {question}\\n\\n Output:\\n\"\n    \n    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\n    output_ids = model.generate(\n        input_ids, \n        max_length=256,\n        temperature=0.2,\n        top_p=0.9,\n        do_sample=True,\n        eos_token_id=tokenizer.eos_token_id  # ✅ Dừng khi gặp eos_token\n    )\n    \n    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n    # Cắt bỏ phần dư\n    output_text = output_text.split(\"Output:\")[-1].strip()\n    output_text = output_text.split(\"Input:\")[0].strip()  # ✅ Xóa phần dư thừa\n\n    return output_text\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query1 = generate_query(\"Ai là người sáng lập ra Open AI?\")\nquery2 = generate_query(\"Con của chồng Ranavalona I là ai?\")\nquery3 = generate_query(\"Giáo viên hướng dẫn Shigeno Yasutsugu là ai?\")\n\nprint(\"📌 Truy vấn 1:\", query1)\nprint(\"📌 Truy vấn 2:\", query2)\nprint(\"📌 Truy vấn 3:\", query3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\noutput_dir = \"/kaggle/working/model\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Lưu mô hình\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"✅ Mô hình đã được lưu tại: {output_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!tar -czvf vinallama7b_model.tar.gz -C /kaggle/working model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!!kaggle kernels output topunguyen/chatkbqa-vi -p /kaggle/working/\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}