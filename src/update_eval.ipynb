{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TOPU\\anaconda3\\envs\\py38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\TOPU\\anaconda3\\envs\\py38\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\TOPU\\.cache\\huggingface\\hub\\models--VoVanPhuc--sup-SimCSE-VietNamese-phobert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load model & tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\")\n",
    "model = AutoModel.from_pretrained(\"VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\")\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Trích xuất vector embedding từ mô hình\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Dùng mean pooling thay vì pooler_output\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "def find_entity_or_relation(label, label_map, facc1_index, top_k=50, similarity_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Tìm thực thể hoặc quan hệ từ gold maps, SimCSE hoặc FACC1.\n",
    "    \"\"\"\n",
    "    label_lower = label.lower()\n",
    "\n",
    "    # Nếu có trong label_map, trả về ngay\n",
    "    if label_map and label_lower in label_map:\n",
    "        return label_map[label_lower]\n",
    "\n",
    "    # Lấy embedding cho label\n",
    "    label_embedding = get_embedding(label_lower).reshape(1, -1)\n",
    "\n",
    "    if label_map:\n",
    "        label_keys = list(label_map.keys())\n",
    "        label_embeddings = np.array([get_embedding(k) for k in label_keys]).squeeze(1)\n",
    "\n",
    "        # Đảm bảo đúng shape\n",
    "        if len(label_embeddings.shape) == 1:\n",
    "            label_embeddings = label_embeddings.reshape(1, -1)\n",
    "\n",
    "        # Tính cosine similarity\n",
    "        similarities = cosine_similarity(label_embedding, label_embeddings).flatten()\n",
    "\n",
    "        # Chọn thực thể gần nhất\n",
    "        merged_list = list(zip(label_keys, similarities))\n",
    "        sorted_list = sorted(merged_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if sorted_list and sorted_list[0][1] > similarity_threshold:\n",
    "            return label_map[sorted_list[0][0]]\n",
    "\n",
    "    # Nếu không tìm thấy, thử trong KB (FACC1)\n",
    "    facc1_cand_entities = facc1_index.get_indexrange_entity_el_pro_one_mention(label_lower, top_k=top_k)\n",
    "    if facc1_cand_entities:\n",
    "        best_match = max(facc1_cand_entities.items(), key=lambda x: x[1])\n",
    "        return best_match[0]\n",
    "\n",
    "    return label  # Trả về label nếu không tìm thấy\n",
    "\n",
    "def parse_nsexpr(expr):\n",
    "    \"\"\"\n",
    "    Chuyển chuỗi biểu thức thành cây cấu trúc dạng nested list.\n",
    "    Hàm này dùng duyệt ký tự, khi gặp '(' sẽ tìm phần con cho đến khi khớp với ')',\n",
    "    và giữ nguyên nội dung trong ngoặc vuông.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(expr):\n",
    "        if expr[i].isspace():\n",
    "            i += 1\n",
    "        elif expr[i] == '(':\n",
    "            # Tìm phần con của biểu thức trong ngoặc đơn\n",
    "            count = 1\n",
    "            j = i + 1\n",
    "            while j < len(expr) and count > 0:\n",
    "                if expr[j] == '(':\n",
    "                    count += 1\n",
    "                elif expr[j] == ')':\n",
    "                    count -= 1\n",
    "                j += 1\n",
    "            # Đệ quy phân tích phần con (loại bỏ ngoặc bao ngoài)\n",
    "            subtree = parse_nsexpr(expr[i+1:j-1])\n",
    "            tokens.append(subtree)\n",
    "            i = j\n",
    "        elif expr[i] == '[':\n",
    "            # Giữ nguyên nội dung trong ngoặc vuông\n",
    "            j = expr.find(']', i)\n",
    "            if j == -1:\n",
    "                return \"\"\n",
    "                raise ValueError(\"Không tìm thấy dấu ']' kết thúc.\")\n",
    "                \n",
    "            token = expr[i:j+1].strip()\n",
    "            tokens.append(token)\n",
    "            i = j + 1\n",
    "        else:\n",
    "            # Đọc một token cho đến khi gặp khoảng trắng hoặc ngoặc\n",
    "            j = i\n",
    "            while j < len(expr) and (not expr[j].isspace()) and expr[j] not in ['(', ')']:\n",
    "                j += 1\n",
    "            tokens.append(expr[i:j])\n",
    "            i = j\n",
    "    return tokens\n",
    "\n",
    "def collect_labels(tree):\n",
    "    \"\"\"\n",
    "    Duyệt cây cấu trúc (nested list) để thu thập các nhãn của quan hệ và thực thể.\n",
    "    Giả sử:\n",
    "      - Biểu thức JOIN có dạng: [\"JOIN\", relation_part, entity_part]\n",
    "      - Phần relation_part: nếu là list và bắt đầu bằng \"R\", thì phần thứ hai chứa nhãn quan hệ (dạng \"[ label ]\"). \n",
    "        Nếu là chuỗi dạng \"[ label ]\" thì đó cũng là nhãn quan hệ.\n",
    "      - Phần entity_part: nếu là chuỗi dạng \"[ label ]\" thì đó là nhãn thực thể, nếu là list thì xử lý đệ quy.\n",
    "      - Biểu thức AND sẽ có nhiều biểu thức con.\n",
    "    \"\"\"\n",
    "    relations = []\n",
    "    entities = []\n",
    "    \n",
    "    if isinstance(tree, list) and tree:\n",
    "        # Nếu token đầu tiên là JOIN hoặc AND\n",
    "        op = tree[0]\n",
    "        if isinstance(op, str):\n",
    "            op_upper = op.upper()\n",
    "        else:\n",
    "            op_upper = \"\"\n",
    "        \n",
    "        if op_upper == \"JOIN\":\n",
    "            # Xử lý phần quan hệ\n",
    "            if len(tree) >= 2:\n",
    "                rel_part = tree[1]\n",
    "                # Nếu là list dạng [ \"R\", \"[ label ]\" ]\n",
    "                if isinstance(rel_part, list) and len(rel_part) >= 2 and isinstance(rel_part[0], str) and rel_part[0].upper() == \"R\":\n",
    "                    token = rel_part[1]\n",
    "                    if isinstance(token, str) and token.startswith('[') and token.endswith(']'):\n",
    "                        rel_label = token[1:-1].strip()\n",
    "                        relations.append(rel_label)\n",
    "                # Nếu là chuỗi dạng \"[ label ]\"\n",
    "                elif isinstance(rel_part, str) and rel_part.startswith('[') and rel_part.endswith(']'):\n",
    "                    rel_label = rel_part[1:-1].strip()\n",
    "                    relations.append(rel_label)\n",
    "                else:\n",
    "                    # Nếu không đúng định dạng, duyệt đệ quy\n",
    "                    sub_rel, sub_ent = collect_labels(rel_part)\n",
    "                    relations.extend(sub_rel)\n",
    "                    entities.extend(sub_ent)\n",
    "            # Xử lý phần thực thể\n",
    "            if len(tree) >= 3:\n",
    "                ent_part = tree[2]\n",
    "                if isinstance(ent_part, list):\n",
    "                    sub_rel, sub_ent = collect_labels(ent_part)\n",
    "                    relations.extend(sub_rel)\n",
    "                    entities.extend(sub_ent)\n",
    "                elif isinstance(ent_part, str) and ent_part.startswith('[') and ent_part.endswith(']'):\n",
    "                    ent_label = ent_part[1:-1].strip()\n",
    "                    entities.append(ent_label)\n",
    "        elif op_upper == \"AND\":\n",
    "            # Với AND, duyệt tất cả các phần con\n",
    "            for sub in tree[1:]:\n",
    "                sub_rel, sub_ent = collect_labels(sub)\n",
    "                relations.extend(sub_rel)\n",
    "                entities.extend(sub_ent)\n",
    "        else:\n",
    "            # Nếu không phải JOIN hay AND, duyệt tất cả các phần tử nếu chúng là list\n",
    "            for elem in tree:\n",
    "                if isinstance(elem, list):\n",
    "                    sub_rel, sub_ent = collect_labels(elem)\n",
    "                    relations.extend(sub_rel)\n",
    "                    entities.extend(sub_ent)\n",
    "    return relations, entities\n",
    "\n",
    "\n",
    "\n",
    "def extract_entities_and_relations(normed_expr):\n",
    "\n",
    "    if not normed_expr or len(normed_expr) == 0:  # Kiểm tra nếu normed_expr rỗng\n",
    "        return [], []\n",
    "    \n",
    "    if normed_expr[0] != \"(\":\n",
    "        return [], []\n",
    "    \n",
    "    tree = parse_nsexpr(normed_expr)\n",
    "    if tree is None:\n",
    "        return [], []  # Trả về danh sách rỗng nếu parse thất bại\n",
    "    \n",
    "    return collect_labels(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: ['Q16538', 'Q54622175', 'Q951038', 'Q1955703', 'Q2636173', 'Q4540147', 'Q650733']\n",
      "Relation: ['P106']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def find_wikidata_entity(label: str, language: str = \"vi\"):\n",
    "    \"\"\"\n",
    "    Tìm mã thực thể Wikidata từ nhãn, ưu tiên kết quả có nhãn khớp chính xác.\n",
    "    \"\"\"\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"search\": label,\n",
    "        \"language\": language,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    if \"search\" in data and data[\"search\"]:\n",
    "        results = [(item[\"id\"], item[\"label\"], item.get(\"description\", \"\")) for item in data[\"search\"]]\n",
    "        \n",
    "        # Ưu tiên kết quả có nhãn khớp chính xác trước\n",
    "        # results.sort(key=lambda x: (x[1].lower() != label.lower(), len(x[2]) if x[2] else 0), reverse=True)\n",
    "        \n",
    "        return [item[0] for item in results]\n",
    "    return None\n",
    "\n",
    "def find_wikidata_relation(label: str, language: str = \"vi\"):\n",
    "    \"\"\"\n",
    "    Tìm mã quan hệ Wikidata từ nhãn, ưu tiên kết quả có nhãn khớp chính xác.\n",
    "    \"\"\"\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"search\": label,\n",
    "        \"language\": language,\n",
    "        \"type\": \"property\",  # Chỉ tìm quan hệ (property)\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    if \"search\" in data and data[\"search\"]:\n",
    "        results = [(item[\"id\"], item[\"label\"], item.get(\"description\", \"\")) for item in data[\"search\"]]\n",
    "        \n",
    "        # # Ưu tiên kết quả có nhãn khớp chính xác trước\n",
    "        # results.sort(key=lambda x: (x[1].lower() != label.lower(), len(x[2]) if x[2] else 0), reverse=True)\n",
    "        \n",
    "        return [item[0] for item in results]\n",
    "    return None\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "entity = find_wikidata_entity(\"South Park\")\n",
    "\n",
    "relation = find_wikidata_relation(\"nghề nghiệp\")\n",
    "\n",
    "print(\"Entity:\", entity)\n",
    "print(\"Relation:\", relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu kết quả vào extracted_entities_relations.json\n"
     ]
    }
   ],
   "source": [
    "# Đọc file JSON gốc\n",
    "input_file = \"Data\\LC-QuAD2.0\\label_map\\LC-QuAD2.0_test.json\"\n",
    "output_file = \"extracted_entities_relations.json\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "data = data[:50]\n",
    "# Xử lý từng câu nor_s_expr\n",
    "results = []\n",
    "for item in data:\n",
    "    nor_s_expr = item.get(\"nor_s_expr\", \"\")\n",
    "    gold_rel = item.get(\"gold_relation_map\")\n",
    "    gold_ent = item.get(\"gold_entity_map\")\n",
    "    rel, ent = extract_entities_and_relations(nor_s_expr)\n",
    "    r_list =[]\n",
    "    e_list = []\n",
    "    for r in rel:\n",
    "        r_list.append(find_wikidata_relation(r))\n",
    "    for e in ent:\n",
    "        e_list.append(find_wikidata_entity(e))\n",
    "    results.append({\"input\": nor_s_expr, \"relation\":r_list,\"entity\": e_list, \"gold_rel\":gold_rel,\"gold_ent\": gold_ent})\n",
    "\n",
    "# Lưu kết quả ra file mới\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Đã lưu kết quả vào {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
