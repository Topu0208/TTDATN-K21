{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10790954,"sourceType":"datasetVersion","datasetId":6696487},{"sourceId":262549,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":224518,"modelId":246268},{"sourceId":265999,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":227590,"modelId":249363}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:51:28.003988Z","iopub.execute_input":"2025-03-05T07:51:28.004299Z","iopub.status.idle":"2025-03-05T07:51:28.309757Z","shell.execute_reply.started":"2025-03-05T07:51:28.004274Z","shell.execute_reply":"2025-03-05T07:51:28.309058Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vi-data/examples (1).json\n/kaggle/input/vi_model/pytorch/default/1/model/adapter_model.safetensors\n/kaggle/input/vi_model/pytorch/default/1/model/adapter_config.json\n/kaggle/input/vi_model/pytorch/default/1/model/README.md\n/kaggle/input/vi_model/pytorch/default/1/model/tokenizer.json\n/kaggle/input/vi_model/pytorch/default/1/model/tokenizer_config.json\n/kaggle/input/vi_model/pytorch/default/1/model/special_tokens_map.json\n/kaggle/input/vina-llama7b/pytorch/default/1/model/adapter_model.safetensors\n/kaggle/input/vina-llama7b/pytorch/default/1/model/adapter_config.json\n/kaggle/input/vina-llama7b/pytorch/default/1/model/README.md\n/kaggle/input/vina-llama7b/pytorch/default/1/model/tokenizer.json\n/kaggle/input/vina-llama7b/pytorch/default/1/model/tokenizer_config.json\n/kaggle/input/vina-llama7b/pytorch/default/1/model/special_tokens_map.json\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":" !pip install fastapi pydantic transformers peft nest_asyncio pyngrok uvicorn SPARQLWrapper","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:51:28.310679Z","iopub.execute_input":"2025-03-05T07:51:28.311072Z","iopub.status.idle":"2025-03-05T07:51:34.446546Z","shell.execute_reply.started":"2025-03-05T07:51:28.311050Z","shell.execute_reply":"2025-03-05T07:51:34.445669Z"}},"outputs":[{"name":"stdout","text":"Collecting fastapi\n  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.11.0a2)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\nCollecting pyngrok\n  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\nCollecting uvicorn\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nCollecting SPARQLWrapper\n  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\nCollecting starlette<0.47.0,>=0.40.0 (from fastapi)\n  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\nCollecting rdflib>=6.1.1 (from SPARQLWrapper)\n  Downloading rdflib-7.1.3-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nCollecting isodate<1.0.0,>=0.7.2 (from rdflib>=6.1.1->SPARQLWrapper)\n  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.0)\nRequirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (3.7.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\nDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\nDownloading rdflib-7.1.3-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.9/564.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading starlette-0.46.0-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\nInstalling collected packages: uvicorn, pyngrok, isodate, starlette, rdflib, SPARQLWrapper, fastapi\nSuccessfully installed SPARQLWrapper-2.0.0 fastapi-0.115.11 isodate-0.7.2 pyngrok-7.2.3 rdflib-7.1.3 starlette-0.46.0 uvicorn-0.34.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel, PeftConfig\nimport torch\nimport nest_asyncio\nfrom pyngrok import ngrok\nimport uvicorn\n\n# Khởi động FastAPI\napp = FastAPI()\n\n# Đường dẫn tới thư mục chứa mô hình trên Kaggle\nmodel_path = \"/kaggle/input/vi_model/pytorch/default/1/model\"\n\n# Load Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load Base Model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\", \n    torch_dtype=torch.float16\n)\n\n# Load cấu hình của LoRA Adapter\npeft_config = PeftConfig.from_pretrained(model_path)\n\n# Load LoRA Adapter\nmodel = PeftModel.from_pretrained(\n    base_model, \n    model_path, \n    is_trainable=True, \n    config=peft_config\n)\n\nprint(\"LoRA Adapter loaded successfully!\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:51:34.448536Z","iopub.execute_input":"2025-03-05T07:51:34.448802Z","iopub.status.idle":"2025-03-05T07:53:16.067728Z","shell.execute_reply.started":"2025-03-05T07:51:34.448778Z","shell.execute_reply":"2025-03-05T07:53:16.066544Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/671 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fce495f541a4982a75e8371201c6d2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02fb0931e75d4ea4a7bd29eaf4aac78a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98a3960512094086a35d4b5b0f0cd43e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba6de48c917f4e1e8e4d7da638d82415"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.80G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7c5849621ae4737bf3e87540622eb2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8e5c1ba0d88452db9c279cf425f5fff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d6fff2e64764c81805f8b23a2c2cf1c"}},"metadata":{}},{"name":"stdout","text":"LoRA Adapter loaded successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import re\n\ndef generate_query(question):\n    input_text = f\"### Instruction:\\nTạo truy vấn Logical Form để lấy thông tin tương ứng với câu hỏi đã cho. \\n\\n### Input:\\nQuestion: {question}\\n\\n### Output:\\n\"\n    \n    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n    \n    output_ids = model.generate(\n        input_ids, \n        max_new_tokens=100,\n        num_beams=5,              \n        num_beam_groups=1,        \n        num_return_sequences=5,    \n        early_stopping=True,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    \n    def clean_output(text):\n        text = text.split(\"Output:\")[-1].strip()  \n        text = re.split(r\"\\n+\", text)[0]  \n        return text\n\n    output_texts = [clean_output(tokenizer.decode(out, skip_special_tokens=True)) for out in output_ids]\n    \n    return output_texts  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:53:16.069476Z","iopub.execute_input":"2025-03-05T07:53:16.070099Z","iopub.status.idle":"2025-03-05T07:53:16.075460Z","shell.execute_reply.started":"2025-03-05T07:53:16.070073Z","shell.execute_reply":"2025-03-05T07:53:16.074538Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import re\n\ndef convert_s_expression_to_sparql_1(s_expression):\n    pattern = r\"\\( ASK \\( AND \\( (\\w+) (\\w+) (\\w+) \\) \\( \\1 \\2 (\\w+) \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        entity, predicate, obj1, obj2 = match.groups()\n        return f\"ASK WHERE {{ wd:{entity} wdt:{predicate} wd:{obj1} . wd:{entity} wdt:{predicate} wd:{obj2} }}\"\n    \n    return None\ndef convert_s_expression_to_sparql_2(s_expression):\n    pattern = r\"\\( MAX \\( JOIN \\( R (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel2, rel1, type_entity = match.groups()\n        return f\"SELECT ?ent WHERE {{ ?ent wdt:{rel1} wd:{type_entity} . ?ent wdt:{rel2} ?obj }} ORDER BY DESC(?obj) LIMIT 5\"\n    \n    return None\ndef convert_s_expression_to_sparql_3(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel2, rel1, entity = match.groups()\n        return f\"SELECT ?answer WHERE {{ wd:{entity} wdt:{rel1} ?X . ?X wdt:{rel2} ?answer }}\"\n    \n    return None\ndef convert_s_expression_to_sparql_4(s_expression):\n    pattern = r\"\\( AND \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel1, en1, rel2, en2 = match.groups()\n        return f\"SELECT DISTINCT ?obj WHERE {{ wd:{en1} wdt:{rel1} ?obj . ?obj wdt:{rel2} wd:{en2} }}\"\n    \n    return None\ndef convert_s_expression_to_sparql_5(s_expression):\n    pattern = r\"\\( MIN \\( JOIN \\( R (\\w+) \\) \\( AND \\( JOIN (\\w+) (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel, rel1, obj1, rel2, obj2 = match.groups()\n        return f\"SELECT ?ent WHERE {{ ?ent wdt:{rel1} wd:{obj1} . ?ent wdt:{rel} ?obj . ?ent wdt:{rel2} wd:{obj2} }} ORDER BY ASC(?obj) LIMIT 5\"\n    \n    return None\ndef convert_s_expression_to_sparql_6(s_expression):\n    pattern = r\"\\( MAX \\( JOIN \\( R (\\w+) \\) \\( AND \\( JOIN (\\w+) (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel, rel1, obj1, rel2, obj2 = match.groups()\n        return f\"SELECT ?ent WHERE {{ ?ent wdt:{rel1} wd:{obj1} . ?ent wdt:{rel} ?obj . ?ent wdt:{rel2} wd:{obj2} }} ORDER BY DESC(?obj) LIMIT 5\"\n    \n    return None\ndef convert_s_expression_to_sparql_7(s_expression):\n    pattern = r\"\\( JOIN (\\w+) (\\w+) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        predicate, obj = match.groups()\n        return f\"SELECT DISTINCT ?answer WHERE {{ ?answer wdt:{predicate} wd:{obj} }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_8(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) \\( AND \\( JOIN \\( R \\1 \\) (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        pred, entity, qualifier, qual_obj = match.groups()\n        return f\"SELECT ?obj WHERE {{ wd:{entity} p:{pred} ?s . ?s ps:{pred} ?obj . ?s pq:{qualifier} wd:{qual_obj} }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_9(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) \\( AND \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\( JOIN \\2 (\\w+) \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        qualifier, pred, entity, obj = match.groups()\n        return f\"SELECT ?value WHERE {{ wd:{entity} p:{pred} ?s . ?s ps:{pred} wd:{obj} . ?s pq:{qualifier} ?value }}\"\n    \n    return None\ndef convert_s_expression_to_sparql_10(s_expression):\n    pattern = r\"\\( AND \\( JOIN (\\w+) (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        pred, obj, p2, type_obj = match.groups()\n        return f\"SELECT DISTINCT ?sbj WHERE {{ ?sbj wdt:{pred} wd:{obj} . ?sbj wdt:{p2} wd:{type_obj} }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_11(s_expression):\n    pattern = r\"\\( AND \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel1, entity, rel2, obj = match.groups()\n        return f\"SELECT ?answer WHERE {{ wd:{entity} wdt:{rel1} ?answer . ?answer wdt:{rel2} wd:{obj} }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_12(s_expression):\n    pattern = r\"\\( ASK \\( (\\=|>=|<=|>|<) \\( JOIN \\( R (\\w+) \\) (\\w+) \\) (\\d+(\\.\\d+)?) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        operator, predicate, entity, value, _ = match.groups()\n        return f\"ASK WHERE {{ wd:{entity} wdt:{predicate} ?obj filter(?obj {operator} {value}) }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_13(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\( JOIN \\( R (\\w+) \\) \\2 \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        pred1, entity, pred2 = match.groups()\n        return f\"SELECT ?ans_1 ?ans_2 WHERE {{ wd:{entity} wdt:{pred1} ?ans_1 . wd:{entity} wdt:{pred2} ?ans_2 }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_14(s_expression):\n    pattern = r\"\\( COUNT \\( JOIN (\\w+) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        predicate, obj = match.groups()\n        return f\"SELECT (COUNT(?sub) AS ?value ) {{ ?sub wdt:{predicate} wd:{obj} }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_15(s_expression):\n    pattern = r\"\\( COUNT \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        predicate, entity = match.groups()\n        return f\"SELECT (COUNT(?obj) AS ?value ) {{ wd:{entity} wdt:{predicate} ?obj }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_16(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) (\\w+) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        predicate, entity = match.groups()\n        return f\"SELECT DISTINCT ?answer WHERE {{ wd:{entity} wdt:{predicate} ?answer }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_17(s_expression):\n    pattern = r\"\\( CHAR \\( AND \\( JOIN (\\w+) (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\) ['\\\"](.*?)['\\\"] \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        pred1, obj1, pred2, obj2, char = match.groups()\n        return f\"\"\"SELECT DISTINCT ?sbj ?sbj_label WHERE {{ \n            ?sbj wdt:{pred1} wd:{obj1} . \n            ?sbj wdt:{pred2} wd:{obj2} . \n            ?sbj rdfs:label ?sbj_label . \n            FILTER(STRSTARTS(lcase(?sbj_label), '{char}')) . \n            FILTER (lang(?sbj_label) = 'en') \n        }} LIMIT 25\"\"\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_18(s_expression):\n    pattern = r\"\\( ASK \\( (\\w+) (\\w+) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        entity, predicate, obj = match.groups()\n        return f\"ASK WHERE {{ wd:{entity} wdt:{predicate} wd:{obj} }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_19(s_expression):\n    pattern = r\"\\( WORD \\( JOIN (\\w+) (\\w+) \\) ['\\\"](.*?)['\\\"] \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        predicate, obj, word = match.groups()\n        return f\"\"\"SELECT DISTINCT ?sbj ?sbj_label WHERE {{ \n            ?sbj wdt:{predicate} wd:{obj} . \n            ?sbj rdfs:label ?sbj_label . \n            FILTER(CONTAINS(lcase(?sbj_label), '{word}')) . \n            FILTER (lang(?sbj_label) = 'en') \n        }} LIMIT 25\"\"\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_20(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel2, rel1, entity = match.groups()\n        return f\"SELECT ?answer WHERE {{ wd:{entity} wdt:{rel1} ?X . ?X wdt:{rel2} ?answer }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_21(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) \\( FILTER \\( JOIN \\( R (\\w+) \\) \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\) \\( ['\\\"](.*?)['\\\"] \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        pred1, pred2, pred3, entity, value = match.groups()\n        return f\"\"\"SELECT ?obj WHERE {{ \n            wd:{entity} p:{pred1} ?s . \n            ?s ps:{pred1} ?obj . \n            ?s pq:{pred2} ?x filter(contains(?x,'{value}')) \n        }}\"\"\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_22(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) \\( FILTER \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\( ['\\\"](.*?)['\\\"] \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel1, rel2, entity, value = match.groups()\n        return f\"\"\"SELECT ?answer WHERE {{ \n            wd:{entity} wdt:{rel1} ?answer . \n            ?answer wdt:{rel2} ?x FILTER(contains(?x,'{value}')) \n        }}\"\"\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_23(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) \\( FILTER \\( JOIN \\( R (\\w+) \\) \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\) \\( ['\\\"](.*?)['\\\"] \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        pred1, pred2, pred3, entity, value = match.groups()\n        return f\"\"\"SELECT ?obj WHERE {{ \n            wd:{entity} p:{pred3} ?s . \n            ?s ps:{pred3} ?obj . \n            ?s pq:{pred2} ?x filter(contains(YEAR(?x),'{value}')) \n        }}\"\"\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_24(s_expression):\n    pattern = r\"\\( FILTER \\( JOIN \\( R (\\w+) \\) \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\) \\( YEAR ['\\\"](.*?)['\\\"] \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel2, rel1, entity, value = match.groups()\n        return f\"\"\"SELECT ?answer WHERE {{ \n            wd:{entity} wdt:{rel1} ?answer . \n            ?answer wdt:{rel2} ?x FILTER(contains(YEAR(?x),'{value}')) \n        }}\"\"\"\n    \n    return None\n\n\ndef convert_s_expression_to_sparql_25(s_expression):\n    pattern = r\"\\( WORD \\( JOIN (\\w+) (\\w+) \\) ['\\\"](.*?)['\\\"] \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        predicate, obj, word = match.groups()\n        return f\"\"\"SELECT DISTINCT ?sbj ?sbj_label WHERE {{ \n            ?sbj wdt:{predicate} wd:{obj} . \n            ?sbj rdfs:label ?sbj_label . \n            FILTER(STRSTARTS(lcase(?sbj_label), '{word}')) . \n            FILTER (lang(?sbj_label) = 'en') \n        }} LIMIT 25\"\"\"\n    \n    return None\n\nconversion_functions = [\n    convert_s_expression_to_sparql_1, convert_s_expression_to_sparql_2, convert_s_expression_to_sparql_3,\n    convert_s_expression_to_sparql_4, convert_s_expression_to_sparql_5, convert_s_expression_to_sparql_6,\n    convert_s_expression_to_sparql_7, convert_s_expression_to_sparql_8, convert_s_expression_to_sparql_9,\n    convert_s_expression_to_sparql_10, convert_s_expression_to_sparql_11, convert_s_expression_to_sparql_12,\n    convert_s_expression_to_sparql_13, convert_s_expression_to_sparql_14, convert_s_expression_to_sparql_15,\n    convert_s_expression_to_sparql_16, convert_s_expression_to_sparql_17, convert_s_expression_to_sparql_18,\n    convert_s_expression_to_sparql_19, convert_s_expression_to_sparql_20, convert_s_expression_to_sparql_21,\n    convert_s_expression_to_sparql_22, convert_s_expression_to_sparql_23, convert_s_expression_to_sparql_24,\n    convert_s_expression_to_sparql_25\n]\n\ndef convert_s_expression_to_sparql(s_expression):\n    \"\"\"\n    Chuyển đổi S-expression sang SPARQL bằng cách thử từng template phù hợp.\n    \"\"\"\n    for func in conversion_functions:\n        sparql_query = func(s_expression)\n        if sparql_query:\n            return sparql_query  # Trả về SPARQL nếu có kết quả hợp lệ\n\n    return \"UNKNOWN\"  # Trả về UNKNOWN nếu không khớp với bất kỳ mẫu nào\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:53:16.077129Z","iopub.execute_input":"2025-03-05T07:53:16.077328Z","iopub.status.idle":"2025-03-05T07:53:16.100794Z","shell.execute_reply.started":"2025-03-05T07:53:16.077310Z","shell.execute_reply":"2025-03-05T07:53:16.099735Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from itertools import product\nfrom SPARQLWrapper import SPARQLWrapper, JSON\nfrom tqdm import tqdm\nimport re\nimport requests\nfrom sklearn.preprocessing import normalize\n\nimport time\n\ndef is_valid_expression(expr):\n    \"\"\"Kiểm tra tính hợp lệ của biểu thức bằng cách đếm số ngoặc mở và đóng.\"\"\"\n    count = 0\n    for char in expr:\n        if char == '(':\n            count += 1\n        elif char == ')':\n            count -= 1\n        if count < 0:\n            return False  # Gặp ngoặc đóng trước ngoặc mở\n    return count == 0\n\ndef search_wikidata_entities(label: str, language: str = \"vi\"):\n    \"\"\"\n    Tìm các thực thể Wikidata có mã Q cụ thể liên quan đến nhãn.\n    \"\"\"\n    url = \"https://www.wikidata.org/w/api.php\"\n    params = {\n        \"action\": \"wbsearchentities\",\n        \"search\": label,\n        \"language\": language,\n        \"type\": \"item\",  # Chỉ tìm thực thể (bỏ qua quan hệ P)\n        \"format\": \"json\"\n    }\n\n    try:\n        response = requests.get(url, params=params, timeout=5)\n        response.raise_for_status()\n        data = response.json()\n    except requests.RequestException as e:\n        print(f\"Lỗi truy vấn Wikidata: {e}\")\n        return []\n\n    if \"search\" in data:\n        return [item[\"id\"] for item in data[\"search\"]][:9]  # Lấy mã thực thể Q\n\n    return []\n\ndef search_wikidata_relations(label: str, language: str = \"vi\"):\n    \"\"\"\n    Tìm các mã quan hệ (P...) trên Wikidata dựa vào nhãn.\n    \"\"\"\n    url = \"https://www.wikidata.org/w/api.php\"\n    params = {\n        \"action\": \"wbsearchentities\",\n        \"search\": label,\n        \"language\": language,\n        \"type\": \"property\",  # Chỉ tìm quan hệ (P...)\n        \"format\": \"json\"\n    }\n\n    try:\n        response = requests.get(url, params=params, timeout=5)\n        response.raise_for_status()\n        data = response.json()\n    except requests.RequestException as e:\n        print(f\"Lỗi truy vấn Wikidata: {e}\")\n        return []\n\n    if \"search\" in data:\n        return [item[\"id\"] for item in data[\"search\"]][:9]  # Lấy mã quan hệ P\n\n    return []\n\n\ndef fix_unbalanced_parentheses(expr):\n    \"\"\"Loại bỏ ngoặc đóng dư nếu có.\"\"\"\n    while not is_valid_expression(expr) and expr.endswith(')'):\n        expr = expr[:-1]\n    return expr\n\n\ndef parse_nsexpr(expr):\n    \"\"\"\n    Chuyển chuỗi biểu thức thành cây cấu trúc dạng nested list.\n    Hàm này dùng duyệt ký tự, khi gặp '(' sẽ tìm phần con cho đến khi khớp với ')',\n    và giữ nguyên nội dung trong ngoặc vuông.\n    \"\"\"\n    tokens = []\n    i = 0\n    while i < len(expr):\n        if expr[i].isspace():\n            i += 1\n        elif expr[i] == '(':\n            # Tìm phần con của biểu thức trong ngoặc đơn\n            count = 1\n            j = i + 1\n            while j < len(expr) and count > 0:\n                if expr[j] == '(':\n                    count += 1\n                elif expr[j] == ')':\n                    count -= 1\n                j += 1\n            # Đệ quy phân tích phần con (loại bỏ ngoặc bao ngoài)\n            subtree = parse_nsexpr(expr[i+1:j-1])\n            tokens.append(subtree)\n            i = j\n        elif expr[i] == '[':\n            # Giữ nguyên nội dung trong ngoặc vuông\n            j = expr.find(']', i)\n            if j == -1:\n                return \"\"\n                raise ValueError(\"Không tìm thấy dấu ']' kết thúc.\")\n                \n            token = expr[i:j+1].strip()\n            tokens.append(token)\n            i = j + 1\n        else:\n            # Đọc một token cho đến khi gặp khoảng trắng hoặc ngoặc\n            j = i\n            while j < len(expr) and (not expr[j].isspace()) and expr[j] not in ['(', ')']:\n                j += 1\n            tokens.append(expr[i:j])\n            i = j\n    return tokens\n\ndef collect_labels(tree):\n    \"\"\"\n    Duyệt cây cấu trúc (nested list) để thu thập các nhãn của quan hệ và thực thể.\n    Giả sử:\n      - Biểu thức JOIN có dạng: [\"JOIN\", relation_part, entity_part]\n      - Phần relation_part: nếu là list và bắt đầu bằng \"R\", thì phần thứ hai chứa nhãn quan hệ (dạng \"[ label ]\"). \n        Nếu là chuỗi dạng \"[ label ]\" thì đó cũng là nhãn quan hệ.\n      - Phần entity_part: nếu là chuỗi dạng \"[ label ]\" thì đó là nhãn thực thể, nếu là list thì xử lý đệ quy.\n      - Biểu thức AND sẽ có nhiều biểu thức con.\n    \"\"\"\n    relations = []\n    entities = []\n    \n    if isinstance(tree, list) and tree:\n        # Nếu token đầu tiên là JOIN hoặc AND\n        op = tree[0]\n        if isinstance(op, str):\n            op_upper = op.upper()\n        else:\n            op_upper = \"\"\n        \n        if op_upper == \"JOIN\":\n            # Xử lý phần quan hệ\n            if len(tree) >= 2:\n                rel_part = tree[1]\n                # Nếu là list dạng [ \"R\", \"[ label ]\" ]\n                if isinstance(rel_part, list) and len(rel_part) >= 2 and isinstance(rel_part[0], str) and rel_part[0].upper() == \"R\":\n                    token = rel_part[1]\n                    if isinstance(token, str) and token.startswith('[') and token.endswith(']'):\n                        rel_label = token[1:-1].strip()\n                        relations.append(rel_label)\n                # Nếu là chuỗi dạng \"[ label ]\"\n                elif isinstance(rel_part, str) and rel_part.startswith('[') and rel_part.endswith(']'):\n                    rel_label = rel_part[1:-1].strip()\n                    relations.append(rel_label)\n                else:\n                    # Nếu không đúng định dạng, duyệt đệ quy\n                    sub_rel, sub_ent = collect_labels(rel_part)\n                    relations.extend(sub_rel)\n                    entities.extend(sub_ent)\n            # Xử lý phần thực thể\n            if len(tree) >= 3:\n                ent_part = tree[2]\n                if isinstance(ent_part, list):\n                    sub_rel, sub_ent = collect_labels(ent_part)\n                    relations.extend(sub_rel)\n                    entities.extend(sub_ent)\n                elif isinstance(ent_part, str) and ent_part.startswith('[') and ent_part.endswith(']'):\n                    ent_label = ent_part[1:-1].strip()\n                    entities.append(ent_label)\n        elif op_upper == \"AND\":\n            # Với AND, duyệt tất cả các phần con\n            for sub in tree[1:]:\n                sub_rel, sub_ent = collect_labels(sub)\n                relations.extend(sub_rel)\n                entities.extend(sub_ent)\n        else:\n            # Nếu không phải JOIN hay AND, duyệt tất cả các phần tử nếu chúng là list\n            for elem in tree:\n                if isinstance(elem, list):\n                    sub_rel, sub_ent = collect_labels(elem)\n                    relations.extend(sub_rel)\n                    entities.extend(sub_ent)\n    return relations, entities\n\n\n\ndef extract_entities_and_relations(normed_expr):\n\n    if not normed_expr or len(normed_expr) == 0:  # Kiểm tra nếu normed_expr rỗng\n        return [], []\n    \n    if normed_expr[0] != \"(\":\n        return [], []\n    \n    tree = parse_nsexpr(normed_expr)\n    if tree is None:\n        return [], []  # Trả về danh sách rỗng nếu parse thất bại\n    \n    return collect_labels(tree)\n\n\nclass SExpressionParser:\n    def __init__(self):\n        self.var_counter = 1  # Đếm số biến trung gian (?X1, ?X2, ...)\n\n    def get_new_var(self):\n        \"\"\"Tạo biến trung gian mới.\"\"\"\n        var_name = f\"?X{self.var_counter}\"\n        self.var_counter += 1\n        return var_name\n\n    def parse_s_expr(self, s_expr):\n        \"\"\"Chuyển đổi S-Expression thành danh sách lồng nhau.\"\"\"\n        s_expr = re.sub(r'\\(', ' ( ', s_expr)\n        s_expr = re.sub(r'\\)', ' ) ', s_expr)\n        tokens = s_expr.split()\n        return self.build_tree(tokens)\n\n    def build_tree(self, tokens):\n        \"\"\"Chuyển đổi danh sách token thành cây lồng nhau.\"\"\"\n        if not tokens:\n            return None\n        token = tokens.pop(0)\n        if token == \"(\":\n            sub_expr = []\n            while tokens[0] != \")\":\n                sub_expr.append(self.build_tree(tokens))\n            tokens.pop(0)  # Bỏ dấu \")\"\n            return sub_expr\n        elif token == \")\":\n            raise ValueError(\"Unexpected ')'\")\n        else:\n            return token\n\n    def process_join(self, expr, target_var):\n        \"\"\"\n        Xử lý JOIN, tạo triple SPARQL.\n        \"\"\"\n        triples = []\n        if not isinstance(expr, list):\n            return expr, triples\n\n        if expr[0] == \"AND\":\n            # Xử lý từng JOIN trong AND riêng lẻ\n            for sub_expr in expr[1:]:\n                _, sub_triples = self.process_join(sub_expr, target_var)\n                triples.extend(sub_triples)\n            return target_var, triples\n\n        if expr[0] == \"JOIN\":\n            right_expr = expr[2]\n            right_triples = []\n            if isinstance(right_expr, list) and right_expr[0] == \"JOIN\":\n                right_var, right_triples = self.process_join(right_expr, self.get_new_var())\n            else: \n                right_var = right_expr\n            \n            # Xử lý nhánh trái\n            left_expr = expr[1]\n            if isinstance(left_expr, list) and left_expr[0] == \"R\":\n                rel = left_expr[1]\n                right = right_var\n                left = target_var\n                if right[0] != '?':\n                    right = \"wd:\" + right\n                if left[0] !='?':\n                    left = \"wd:\" + left   \n                triples.append([right, f\"wdt:{rel}\", left])\n            else:\n                right = right_var\n                left = target_var\n                if right[0] != '?':\n                    right = \"wd:\" + right\n                if left[0] !='?':\n                    left = \"wd:\" + left   \n                triples.append([left, f\"wdt:{left_expr}\", right])\n\n            # Thêm các triples từ nhánh phải trước khi thêm triple chính\n            triples = right_triples + triples\n            return target_var, triples\n\n        return expr, triples\n\n    def s_expr_to_sparql(self, s_expr):\n        \"\"\"Chuyển đổi từ S-Expression sang SPARQL.\"\"\"\n        if s_expr.count(\"(\") != s_expr.count(\")\"):\n            return None\n        if s_expr.count(\"[\") != s_expr.count(\"]\"):\n            return None\n        parsed_expr = self.parse_s_expr(s_expr)\n        target_var = \"?answer\"\n        final_var, triples = self.process_join(parsed_expr, target_var)\n\n        sparql_body = \"\\n  \".join([\" \".join(t) + \" .\" for t in triples])\n        sparql_query = f\"\"\"PREFIX wd: <http://www.wikidata.org/entity/> \nPREFIX wdt: <http://www.wikidata.org/prop/direct/> \nSELECT DISTINCT {target_var} WHERE {{ \n  {sparql_body}\n}}\"\"\"\n        return sparql_query\n\nWIKIDATA_SPARQL_ENDPOINT = \"https://query.wikidata.org/sparql\"\nWIKIDATA_API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n\ndef execute_query_with_odbc(sparql_query):\n    \"\"\"Truy vấn Wikidata và trả về danh sách câu trả lời (bao gồm tất cả biến)\"\"\"\n    headers = {\"User-Agent\": \"Mozilla/5.0\", \"Accept\": \"application/sparql-results+json\"}\n    response = requests.get(WIKIDATA_SPARQL_ENDPOINT, params={\"query\": sparql_query, \"format\": \"json\"}, headers=headers)\n\n    if response.status_code == 200:\n        results = response.json().get(\"results\", {}).get(\"bindings\", [])\n        answers = []\n\n        for result in results:\n            for var in result:  # Duyệt qua tất cả các biến trả về\n                value = result[var][\"value\"]\n                answers.append(value)  # Chấp nhận tất cả giá trị, không chỉ thực thể Wikidata\n\n        return answers  # Trả về toàn bộ danh sách kết quả\n\n    return []\n\ndef convert_normed_to_s_expression(normed_expr): #, gold_entity_map, gold_relation_map\n    \"\"\"\n    Chuyển đổi từ normed_sexpression sang s_expression.\n    Sau khi trích xuất các nhãn quan hệ và thực thể từ normed_expr,\n    ta lấy danh sách các mã ứng viên cho mỗi nhãn và tạo hoán vị giữa các cặp ứng viên đó.\n    Kết quả trả về là một danh sách các s_expression khả dĩ.\n    \"\"\"\n    normed_expr = fix_unbalanced_parentheses(normed_expr)\n    # Trích xuất các nhãn quan hệ và thực thể từ biểu thức\n    relations, entities = extract_entities_and_relations(normed_expr)\n    # Tạo mapping từ token xuất hiện trong biểu thức sang danh sách các ứng viên mã.\n    # Ví dụ: token_str = \"[ author ]\"\n    s_expressions = []\n    candidate_map = {}\n    for rel in relations:\n       \n        token = f'[ {rel} ]'\n        candidate = search_wikidata_relations(str(rel))\n\n        # if not set(candidate)&set(gold_relation_map):\n        #     return s_expressions \n        # Nếu candidate không phải danh sách, chuyển nó thành danh sách để tạo hoán vị\n        if not isinstance(candidate, list):\n            candidate = [candidate]\n        candidate_map[token] = candidate\n       \n    for ent in entities:\n        token = f'[ {ent} ]'\n        candidate = search_wikidata_entities(str(ent))\n        # if not set(candidate)&set(gold_entity_map):\n        #     return s_expressions \n        if not isinstance(candidate, list):\n            candidate = [candidate]\n        candidate_map[token] = candidate\n    \n    # Nếu không có token nào cần thay thế, trả về biểu thức gốc\n\n    if not candidate_map:\n        return [normed_expr]\n    \n    # Lấy danh sách các token và danh sách các danh sách ứng viên tương ứng\n    tokens = list(candidate_map.keys())\n    candidate_lists = [candidate_map[token] for token in tokens]\n    \n    # Tạo tất cả các hoán vị ứng viên (Cartesian product)\n    all_combinations = list(product(*candidate_lists))\n    \n    for comb in all_combinations:\n        temp_expr = normed_expr\n        # Với mỗi token, thay thế bằng ứng viên tương ứng theo hoán vị\n        for token, replacement in zip(tokens, comb):\n            temp_expr = temp_expr.replace(token, replacement)\n        s_expressions.append(temp_expr)\n    \n    return s_expressions\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:53:16.101829Z","iopub.execute_input":"2025-03-05T07:53:16.102092Z","iopub.status.idle":"2025-03-05T07:53:16.146681Z","shell.execute_reply.started":"2025-03-05T07:53:16.102058Z","shell.execute_reply":"2025-03-05T07:53:16.145746Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import time\n\ndef run_result(query):\n   \n    query_result = []\n    set_query = convert_normed_to_s_expression(query)\n\n    for q in set_query:\n        sparql = convert_s_expression_to_sparql(q)\n        if sparql == \"UNKNOWN\":\n            continue\n        \n        result = execute_query_with_odbc(sparql)\n\n        # Nếu kết quả không rỗng, thêm vào danh sách\n        if result:\n            query_result.append(result)\n  \n    return query_result  # Trả về danh sách chứa danh sách con (chưa xử lý)\n\n# Xử lý nhiều truy vấn\ndef main(question):\n    start_time = time.time()\n    \n    queries = generate_query(question)\n    all_results = []  # Chứa tất cả kết quả từ các truy vấn\n\n    for i, query in enumerate(queries, 1):\n        print(f\"🔹 Query {i}: {query}\")\n        query_result = run_result(query)\n        all_results.extend(query_result)  # Thêm danh sách kết quả vào all_results\n\n    # Gộp danh sách con và loại bỏ rỗng\n    final_results = sum(all_results, [])\n    end_time = time.time()\n    print(f\"Execution time: {end_time - start_time} seconds\")\n    print(\"📌 Final Results:\", final_results)  # Kết quả cuối cùng\n\n# Chạy thử\nquestion = \"Việt Nam ở châu lục nào?\"\nmain(question)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T08:11:06.712501Z","iopub.execute_input":"2025-03-05T08:11:06.712835Z","iopub.status.idle":"2025-03-05T08:11:14.569334Z","shell.execute_reply.started":"2025-03-05T08:11:06.712807Z","shell.execute_reply":"2025-03-05T08:11:14.568603Z"}},"outputs":[{"name":"stdout","text":"🔹 Query 1: ( JOIN [ lục địa ] [ Việt Nam ] )\n🔹 Query 2: ( JOIN ( R [ nằm trong múi giờ ] ) [ Việt Nam ] )\n🔹 Query 3: ( JOIN ( R [ nằm trong phạm vi của khu vực hành chính ] ) [ Việt Nam ] )\n🔹 Query 4: ( JOIN ( R [ lục địa ] ) [ Việt Nam ] )\n🔹 Query 5: \nExecution time: 7.850265026092529 seconds\n📌 Final Results: ['http://www.wikidata.org/entity/Q63285961', 'http://www.wikidata.org/entity/Q6940', 'http://www.wikidata.org/entity/Q881', 'http://www.wikidata.org/entity/Q48', 'http://www.wikidata.org/entity/Q48', 'http://www.wikidata.org/entity/Q48']\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# # Định nghĩa dữ liệu đầu vào cho API\n# class InputData(BaseModel):\n#     input_text: str\n\n# class QueryInput(BaseModel):\n#     question: str\n    \n# # Tạo endpoint để gọi model\n# @app.post(\"/predict\")\n# def predict(data: InputData):\n#     print(\"Received data:\", data)\n#     try:\n#         # Tokenize input\n#         input_ids = tokenizer.encode(data.input_text, return_tensors='pt').to(model.device)\n#         print(input_ids)\n#         # Generate output từ model\n#         output = model.generate(input_ids, max_length=100)\n#         output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n        \n#         print(\"Generated output:\", output_text)\n#         return {\"output_text\": output_text}\n#     except Exception as e:\n#         print(\"Error during prediction:\", str(e))\n#         return {\"error\": str(e)}\n        \n\n\n# # API generate_query\n# @app.post(\"/generate_query\")\n# def generate_query_api(data: QueryInput):\n#     try:\n#         print(\"Ques: \"+data.question)\n#         queries = generate_query(data.question)\n#         print (\"toiday\")\n       \n#         return {\"queries\": queries}\n#     except Exception as e:\n#         return {\"error\": str(e)}\n\n# # Sử dụng nest_asyncio để tránh lỗi asyncio trên Kaggle\n# nest_asyncio.apply()\n\n# # Thêm authtoken của bạn tại đây\n# ngrok.set_auth_token(\"2tT6aa8W42iNmqXWfcwwAyzzokp_5Ko18vsKiFXBY6mz6sCwP\")\n\n# # Mở ngrok tunnel\n# public_url = ngrok.connect(8000)\n# print(f\"Public URL: {public_url}\")\n\n# # Chạy ứng dụng bằng Uvicorn\n# uvicorn.run(app, host='0.0.0.0', port=8000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:53:17.580867Z","iopub.execute_input":"2025-03-05T07:53:17.581216Z","iopub.status.idle":"2025-03-05T07:53:17.585097Z","shell.execute_reply.started":"2025-03-05T07:53:17.581172Z","shell.execute_reply":"2025-03-05T07:53:17.584153Z"}},"outputs":[],"execution_count":10}]}