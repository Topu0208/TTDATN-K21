{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10790954,"sourceType":"datasetVersion","datasetId":6696487},{"sourceId":262549,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":224518,"modelId":246268},{"sourceId":265999,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":227590,"modelId":249363}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:51:28.003988Z","iopub.execute_input":"2025-03-05T07:51:28.004299Z","iopub.status.idle":"2025-03-05T07:51:28.309757Z","shell.execute_reply.started":"2025-03-05T07:51:28.004274Z","shell.execute_reply":"2025-03-05T07:51:28.309058Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vi-data/examples (1).json\n/kaggle/input/vi_model/pytorch/default/1/model/adapter_model.safetensors\n/kaggle/input/vi_model/pytorch/default/1/model/adapter_config.json\n/kaggle/input/vi_model/pytorch/default/1/model/README.md\n/kaggle/input/vi_model/pytorch/default/1/model/tokenizer.json\n/kaggle/input/vi_model/pytorch/default/1/model/tokenizer_config.json\n/kaggle/input/vi_model/pytorch/default/1/model/special_tokens_map.json\n/kaggle/input/vina-llama7b/pytorch/default/1/model/adapter_model.safetensors\n/kaggle/input/vina-llama7b/pytorch/default/1/model/adapter_config.json\n/kaggle/input/vina-llama7b/pytorch/default/1/model/README.md\n/kaggle/input/vina-llama7b/pytorch/default/1/model/tokenizer.json\n/kaggle/input/vina-llama7b/pytorch/default/1/model/tokenizer_config.json\n/kaggle/input/vina-llama7b/pytorch/default/1/model/special_tokens_map.json\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":" !pip install fastapi pydantic transformers peft nest_asyncio pyngrok uvicorn SPARQLWrapper","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:51:28.310679Z","iopub.execute_input":"2025-03-05T07:51:28.311072Z","iopub.status.idle":"2025-03-05T07:51:34.446546Z","shell.execute_reply.started":"2025-03-05T07:51:28.311050Z","shell.execute_reply":"2025-03-05T07:51:34.445669Z"}},"outputs":[{"name":"stdout","text":"Collecting fastapi\n  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.11.0a2)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\nCollecting pyngrok\n  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\nCollecting uvicorn\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nCollecting SPARQLWrapper\n  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\nCollecting starlette<0.47.0,>=0.40.0 (from fastapi)\n  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\nCollecting rdflib>=6.1.1 (from SPARQLWrapper)\n  Downloading rdflib-7.1.3-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nCollecting isodate<1.0.0,>=0.7.2 (from rdflib>=6.1.1->SPARQLWrapper)\n  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.0)\nRequirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (3.7.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\nDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\nDownloading rdflib-7.1.3-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m564.9/564.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading starlette-0.46.0-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\nInstalling collected packages: uvicorn, pyngrok, isodate, starlette, rdflib, SPARQLWrapper, fastapi\nSuccessfully installed SPARQLWrapper-2.0.0 fastapi-0.115.11 isodate-0.7.2 pyngrok-7.2.3 rdflib-7.1.3 starlette-0.46.0 uvicorn-0.34.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel, PeftConfig\nimport torch\nimport nest_asyncio\nfrom pyngrok import ngrok\nimport uvicorn\n\n# Kh·ªüi ƒë·ªông FastAPI\napp = FastAPI()\n\n# ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c ch·ª©a m√¥ h√¨nh tr√™n Kaggle\nmodel_path = \"/kaggle/input/vi_model/pytorch/default/1/model\"\n\n# Load Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load Base Model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\", \n    torch_dtype=torch.float16\n)\n\n# Load c·∫•u h√¨nh c·ªßa LoRA Adapter\npeft_config = PeftConfig.from_pretrained(model_path)\n\n# Load LoRA Adapter\nmodel = PeftModel.from_pretrained(\n    base_model, \n    model_path, \n    is_trainable=True, \n    config=peft_config\n)\n\nprint(\"LoRA Adapter loaded successfully!\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:51:34.448536Z","iopub.execute_input":"2025-03-05T07:51:34.448802Z","iopub.status.idle":"2025-03-05T07:53:16.067728Z","shell.execute_reply.started":"2025-03-05T07:51:34.448778Z","shell.execute_reply":"2025-03-05T07:53:16.066544Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/671 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fce495f541a4982a75e8371201c6d2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02fb0931e75d4ea4a7bd29eaf4aac78a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98a3960512094086a35d4b5b0f0cd43e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba6de48c917f4e1e8e4d7da638d82415"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.80G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7c5849621ae4737bf3e87540622eb2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8e5c1ba0d88452db9c279cf425f5fff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d6fff2e64764c81805f8b23a2c2cf1c"}},"metadata":{}},{"name":"stdout","text":"LoRA Adapter loaded successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import re\n\ndef generate_query(question):\n    input_text = f\"### Instruction:\\nT·∫°o truy v·∫•n Logical Form ƒë·ªÉ l·∫•y th√¥ng tin t∆∞∆°ng ·ª©ng v·ªõi c√¢u h·ªèi ƒë√£ cho. \\n\\n### Input:\\nQuestion: {question}\\n\\n### Output:\\n\"\n    \n    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n    \n    output_ids = model.generate(\n        input_ids, \n        max_new_tokens=100,\n        num_beams=5,              \n        num_beam_groups=1,        \n        num_return_sequences=5,    \n        early_stopping=True,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    \n    def clean_output(text):\n        text = text.split(\"Output:\")[-1].strip()  \n        text = re.split(r\"\\n+\", text)[0]  \n        return text\n\n    output_texts = [clean_output(tokenizer.decode(out, skip_special_tokens=True)) for out in output_ids]\n    \n    return output_texts  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:53:16.069476Z","iopub.execute_input":"2025-03-05T07:53:16.070099Z","iopub.status.idle":"2025-03-05T07:53:16.075460Z","shell.execute_reply.started":"2025-03-05T07:53:16.070073Z","shell.execute_reply":"2025-03-05T07:53:16.074538Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import re\n\ndef convert_s_expression_to_sparql_1(s_expression):\n    pattern = r\"\\( ASK \\( AND \\( (\\w+) (\\w+) (\\w+) \\) \\( \\1 \\2 (\\w+) \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        entity, predicate, obj1, obj2 = match.groups()\n        return f\"ASK WHERE {{ wd:{entity} wdt:{predicate} wd:{obj1} . wd:{entity} wdt:{predicate} wd:{obj2} }}\"\n    \n    return None\ndef convert_s_expression_to_sparql_2(s_expression):\n    pattern = r\"\\( MAX \\( JOIN \\( R (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel2, rel1, type_entity = match.groups()\n        return f\"SELECT ?ent WHERE {{ ?ent wdt:{rel1} wd:{type_entity} . ?ent wdt:{rel2} ?obj }} ORDER BY DESC(?obj) LIMIT 5\"\n    \n    return None\ndef convert_s_expression_to_sparql_3(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel2, rel1, entity = match.groups()\n        return f\"SELECT ?answer WHERE {{ wd:{entity} wdt:{rel1} ?X . ?X wdt:{rel2} ?answer }}\"\n    \n    return None\ndef convert_s_expression_to_sparql_4(s_expression):\n    pattern = r\"\\( AND \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel1, en1, rel2, en2 = match.groups()\n        return f\"SELECT DISTINCT ?obj WHERE {{ wd:{en1} wdt:{rel1} ?obj . ?obj wdt:{rel2} wd:{en2} }}\"\n    \n    return None\ndef convert_s_expression_to_sparql_5(s_expression):\n    pattern = r\"\\( MIN \\( JOIN \\( R (\\w+) \\) \\( AND \\( JOIN (\\w+) (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel, rel1, obj1, rel2, obj2 = match.groups()\n        return f\"SELECT ?ent WHERE {{ ?ent wdt:{rel1} wd:{obj1} . ?ent wdt:{rel} ?obj . ?ent wdt:{rel2} wd:{obj2} }} ORDER BY ASC(?obj) LIMIT 5\"\n    \n    return None\ndef convert_s_expression_to_sparql_6(s_expression):\n    pattern = r\"\\( MAX \\( JOIN \\( R (\\w+) \\) \\( AND \\( JOIN (\\w+) (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel, rel1, obj1, rel2, obj2 = match.groups()\n        return f\"SELECT ?ent WHERE {{ ?ent wdt:{rel1} wd:{obj1} . ?ent wdt:{rel} ?obj . ?ent wdt:{rel2} wd:{obj2} }} ORDER BY DESC(?obj) LIMIT 5\"\n    \n    return None\ndef convert_s_expression_to_sparql_7(s_expression):\n    pattern = r\"\\( JOIN (\\w+) (\\w+) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        predicate, obj = match.groups()\n        return f\"SELECT DISTINCT ?answer WHERE {{ ?answer wdt:{predicate} wd:{obj} }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_8(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) \\( AND \\( JOIN \\( R \\1 \\) (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        pred, entity, qualifier, qual_obj = match.groups()\n        return f\"SELECT ?obj WHERE {{ wd:{entity} p:{pred} ?s . ?s ps:{pred} ?obj . ?s pq:{qualifier} wd:{qual_obj} }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_9(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) \\( AND \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\( JOIN \\2 (\\w+) \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        qualifier, pred, entity, obj = match.groups()\n        return f\"SELECT ?value WHERE {{ wd:{entity} p:{pred} ?s . ?s ps:{pred} wd:{obj} . ?s pq:{qualifier} ?value }}\"\n    \n    return None\ndef convert_s_expression_to_sparql_10(s_expression):\n    pattern = r\"\\( AND \\( JOIN (\\w+) (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        pred, obj, p2, type_obj = match.groups()\n        return f\"SELECT DISTINCT ?sbj WHERE {{ ?sbj wdt:{pred} wd:{obj} . ?sbj wdt:{p2} wd:{type_obj} }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_11(s_expression):\n    pattern = r\"\\( AND \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel1, entity, rel2, obj = match.groups()\n        return f\"SELECT ?answer WHERE {{ wd:{entity} wdt:{rel1} ?answer . ?answer wdt:{rel2} wd:{obj} }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_12(s_expression):\n    pattern = r\"\\( ASK \\( (\\=|>=|<=|>|<) \\( JOIN \\( R (\\w+) \\) (\\w+) \\) (\\d+(\\.\\d+)?) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        operator, predicate, entity, value, _ = match.groups()\n        return f\"ASK WHERE {{ wd:{entity} wdt:{predicate} ?obj filter(?obj {operator} {value}) }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_13(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\( JOIN \\( R (\\w+) \\) \\2 \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        pred1, entity, pred2 = match.groups()\n        return f\"SELECT ?ans_1 ?ans_2 WHERE {{ wd:{entity} wdt:{pred1} ?ans_1 . wd:{entity} wdt:{pred2} ?ans_2 }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_14(s_expression):\n    pattern = r\"\\( COUNT \\( JOIN (\\w+) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        predicate, obj = match.groups()\n        return f\"SELECT (COUNT(?sub) AS ?value ) {{ ?sub wdt:{predicate} wd:{obj} }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_15(s_expression):\n    pattern = r\"\\( COUNT \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        predicate, entity = match.groups()\n        return f\"SELECT (COUNT(?obj) AS ?value ) {{ wd:{entity} wdt:{predicate} ?obj }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_16(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) (\\w+) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        predicate, entity = match.groups()\n        return f\"SELECT DISTINCT ?answer WHERE {{ wd:{entity} wdt:{predicate} ?answer }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_17(s_expression):\n    pattern = r\"\\( CHAR \\( AND \\( JOIN (\\w+) (\\w+) \\) \\( JOIN (\\w+) (\\w+) \\) \\) ['\\\"](.*?)['\\\"] \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        pred1, obj1, pred2, obj2, char = match.groups()\n        return f\"\"\"SELECT DISTINCT ?sbj ?sbj_label WHERE {{ \n            ?sbj wdt:{pred1} wd:{obj1} . \n            ?sbj wdt:{pred2} wd:{obj2} . \n            ?sbj rdfs:label ?sbj_label . \n            FILTER(STRSTARTS(lcase(?sbj_label), '{char}')) . \n            FILTER (lang(?sbj_label) = 'en') \n        }} LIMIT 25\"\"\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_18(s_expression):\n    pattern = r\"\\( ASK \\( (\\w+) (\\w+) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        entity, predicate, obj = match.groups()\n        return f\"ASK WHERE {{ wd:{entity} wdt:{predicate} wd:{obj} }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_19(s_expression):\n    pattern = r\"\\( WORD \\( JOIN (\\w+) (\\w+) \\) ['\\\"](.*?)['\\\"] \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        predicate, obj, word = match.groups()\n        return f\"\"\"SELECT DISTINCT ?sbj ?sbj_label WHERE {{ \n            ?sbj wdt:{predicate} wd:{obj} . \n            ?sbj rdfs:label ?sbj_label . \n            FILTER(CONTAINS(lcase(?sbj_label), '{word}')) . \n            FILTER (lang(?sbj_label) = 'en') \n        }} LIMIT 25\"\"\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_20(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel2, rel1, entity = match.groups()\n        return f\"SELECT ?answer WHERE {{ wd:{entity} wdt:{rel1} ?X . ?X wdt:{rel2} ?answer }}\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_21(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) \\( FILTER \\( JOIN \\( R (\\w+) \\) \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\) \\( ['\\\"](.*?)['\\\"] \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        pred1, pred2, pred3, entity, value = match.groups()\n        return f\"\"\"SELECT ?obj WHERE {{ \n            wd:{entity} p:{pred1} ?s . \n            ?s ps:{pred1} ?obj . \n            ?s pq:{pred2} ?x filter(contains(?x,'{value}')) \n        }}\"\"\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_22(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) \\( FILTER \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\( ['\\\"](.*?)['\\\"] \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel1, rel2, entity, value = match.groups()\n        return f\"\"\"SELECT ?answer WHERE {{ \n            wd:{entity} wdt:{rel1} ?answer . \n            ?answer wdt:{rel2} ?x FILTER(contains(?x,'{value}')) \n        }}\"\"\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_23(s_expression):\n    pattern = r\"\\( JOIN \\( R (\\w+) \\) \\( FILTER \\( JOIN \\( R (\\w+) \\) \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\) \\( ['\\\"](.*?)['\\\"] \\) \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        pred1, pred2, pred3, entity, value = match.groups()\n        return f\"\"\"SELECT ?obj WHERE {{ \n            wd:{entity} p:{pred3} ?s . \n            ?s ps:{pred3} ?obj . \n            ?s pq:{pred2} ?x filter(contains(YEAR(?x),'{value}')) \n        }}\"\"\"\n    \n    return None\n\ndef convert_s_expression_to_sparql_24(s_expression):\n    pattern = r\"\\( FILTER \\( JOIN \\( R (\\w+) \\) \\( JOIN \\( R (\\w+) \\) (\\w+) \\) \\) \\( YEAR ['\\\"](.*?)['\\\"] \\) \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        rel2, rel1, entity, value = match.groups()\n        return f\"\"\"SELECT ?answer WHERE {{ \n            wd:{entity} wdt:{rel1} ?answer . \n            ?answer wdt:{rel2} ?x FILTER(contains(YEAR(?x),'{value}')) \n        }}\"\"\"\n    \n    return None\n\n\ndef convert_s_expression_to_sparql_25(s_expression):\n    pattern = r\"\\( WORD \\( JOIN (\\w+) (\\w+) \\) ['\\\"](.*?)['\\\"] \\)\"\n    match = re.match(pattern, s_expression)\n    \n    if match:\n        predicate, obj, word = match.groups()\n        return f\"\"\"SELECT DISTINCT ?sbj ?sbj_label WHERE {{ \n            ?sbj wdt:{predicate} wd:{obj} . \n            ?sbj rdfs:label ?sbj_label . \n            FILTER(STRSTARTS(lcase(?sbj_label), '{word}')) . \n            FILTER (lang(?sbj_label) = 'en') \n        }} LIMIT 25\"\"\"\n    \n    return None\n\nconversion_functions = [\n    convert_s_expression_to_sparql_1, convert_s_expression_to_sparql_2, convert_s_expression_to_sparql_3,\n    convert_s_expression_to_sparql_4, convert_s_expression_to_sparql_5, convert_s_expression_to_sparql_6,\n    convert_s_expression_to_sparql_7, convert_s_expression_to_sparql_8, convert_s_expression_to_sparql_9,\n    convert_s_expression_to_sparql_10, convert_s_expression_to_sparql_11, convert_s_expression_to_sparql_12,\n    convert_s_expression_to_sparql_13, convert_s_expression_to_sparql_14, convert_s_expression_to_sparql_15,\n    convert_s_expression_to_sparql_16, convert_s_expression_to_sparql_17, convert_s_expression_to_sparql_18,\n    convert_s_expression_to_sparql_19, convert_s_expression_to_sparql_20, convert_s_expression_to_sparql_21,\n    convert_s_expression_to_sparql_22, convert_s_expression_to_sparql_23, convert_s_expression_to_sparql_24,\n    convert_s_expression_to_sparql_25\n]\n\ndef convert_s_expression_to_sparql(s_expression):\n    \"\"\"\n    Chuy·ªÉn ƒë·ªïi S-expression sang SPARQL b·∫±ng c√°ch th·ª≠ t·ª´ng template ph√π h·ª£p.\n    \"\"\"\n    for func in conversion_functions:\n        sparql_query = func(s_expression)\n        if sparql_query:\n            return sparql_query  # Tr·∫£ v·ªÅ SPARQL n·∫øu c√≥ k·∫øt qu·∫£ h·ª£p l·ªá\n\n    return \"UNKNOWN\"  # Tr·∫£ v·ªÅ UNKNOWN n·∫øu kh√¥ng kh·ªõp v·ªõi b·∫•t k·ª≥ m·∫´u n√†o\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:53:16.077129Z","iopub.execute_input":"2025-03-05T07:53:16.077328Z","iopub.status.idle":"2025-03-05T07:53:16.100794Z","shell.execute_reply.started":"2025-03-05T07:53:16.077310Z","shell.execute_reply":"2025-03-05T07:53:16.099735Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from itertools import product\nfrom SPARQLWrapper import SPARQLWrapper, JSON\nfrom tqdm import tqdm\nimport re\nimport requests\nfrom sklearn.preprocessing import normalize\n\nimport time\n\ndef is_valid_expression(expr):\n    \"\"\"Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa bi·ªÉu th·ª©c b·∫±ng c√°ch ƒë·∫øm s·ªë ngo·∫∑c m·ªü v√† ƒë√≥ng.\"\"\"\n    count = 0\n    for char in expr:\n        if char == '(':\n            count += 1\n        elif char == ')':\n            count -= 1\n        if count < 0:\n            return False  # G·∫∑p ngo·∫∑c ƒë√≥ng tr∆∞·ªõc ngo·∫∑c m·ªü\n    return count == 0\n\ndef search_wikidata_entities(label: str, language: str = \"vi\"):\n    \"\"\"\n    T√¨m c√°c th·ª±c th·ªÉ Wikidata c√≥ m√£ Q c·ª• th·ªÉ li√™n quan ƒë·∫øn nh√£n.\n    \"\"\"\n    url = \"https://www.wikidata.org/w/api.php\"\n    params = {\n        \"action\": \"wbsearchentities\",\n        \"search\": label,\n        \"language\": language,\n        \"type\": \"item\",  # Ch·ªâ t√¨m th·ª±c th·ªÉ (b·ªè qua quan h·ªá P)\n        \"format\": \"json\"\n    }\n\n    try:\n        response = requests.get(url, params=params, timeout=5)\n        response.raise_for_status()\n        data = response.json()\n    except requests.RequestException as e:\n        print(f\"L·ªói truy v·∫•n Wikidata: {e}\")\n        return []\n\n    if \"search\" in data:\n        return [item[\"id\"] for item in data[\"search\"]][:9]  # L·∫•y m√£ th·ª±c th·ªÉ Q\n\n    return []\n\ndef search_wikidata_relations(label: str, language: str = \"vi\"):\n    \"\"\"\n    T√¨m c√°c m√£ quan h·ªá (P...) tr√™n Wikidata d·ª±a v√†o nh√£n.\n    \"\"\"\n    url = \"https://www.wikidata.org/w/api.php\"\n    params = {\n        \"action\": \"wbsearchentities\",\n        \"search\": label,\n        \"language\": language,\n        \"type\": \"property\",  # Ch·ªâ t√¨m quan h·ªá (P...)\n        \"format\": \"json\"\n    }\n\n    try:\n        response = requests.get(url, params=params, timeout=5)\n        response.raise_for_status()\n        data = response.json()\n    except requests.RequestException as e:\n        print(f\"L·ªói truy v·∫•n Wikidata: {e}\")\n        return []\n\n    if \"search\" in data:\n        return [item[\"id\"] for item in data[\"search\"]][:9]  # L·∫•y m√£ quan h·ªá P\n\n    return []\n\n\ndef fix_unbalanced_parentheses(expr):\n    \"\"\"Lo·∫°i b·ªè ngo·∫∑c ƒë√≥ng d∆∞ n·∫øu c√≥.\"\"\"\n    while not is_valid_expression(expr) and expr.endswith(')'):\n        expr = expr[:-1]\n    return expr\n\n\ndef parse_nsexpr(expr):\n    \"\"\"\n    Chuy·ªÉn chu·ªói bi·ªÉu th·ª©c th√†nh c√¢y c·∫•u tr√∫c d·∫°ng nested list.\n    H√†m n√†y d√πng duy·ªát k√Ω t·ª±, khi g·∫∑p '(' s·∫Ω t√¨m ph·∫ßn con cho ƒë·∫øn khi kh·ªõp v·ªõi ')',\n    v√† gi·ªØ nguy√™n n·ªôi dung trong ngo·∫∑c vu√¥ng.\n    \"\"\"\n    tokens = []\n    i = 0\n    while i < len(expr):\n        if expr[i].isspace():\n            i += 1\n        elif expr[i] == '(':\n            # T√¨m ph·∫ßn con c·ªßa bi·ªÉu th·ª©c trong ngo·∫∑c ƒë∆°n\n            count = 1\n            j = i + 1\n            while j < len(expr) and count > 0:\n                if expr[j] == '(':\n                    count += 1\n                elif expr[j] == ')':\n                    count -= 1\n                j += 1\n            # ƒê·ªá quy ph√¢n t√≠ch ph·∫ßn con (lo·∫°i b·ªè ngo·∫∑c bao ngo√†i)\n            subtree = parse_nsexpr(expr[i+1:j-1])\n            tokens.append(subtree)\n            i = j\n        elif expr[i] == '[':\n            # Gi·ªØ nguy√™n n·ªôi dung trong ngo·∫∑c vu√¥ng\n            j = expr.find(']', i)\n            if j == -1:\n                return \"\"\n                raise ValueError(\"Kh√¥ng t√¨m th·∫•y d·∫•u ']' k·∫øt th√∫c.\")\n                \n            token = expr[i:j+1].strip()\n            tokens.append(token)\n            i = j + 1\n        else:\n            # ƒê·ªçc m·ªôt token cho ƒë·∫øn khi g·∫∑p kho·∫£ng tr·∫Øng ho·∫∑c ngo·∫∑c\n            j = i\n            while j < len(expr) and (not expr[j].isspace()) and expr[j] not in ['(', ')']:\n                j += 1\n            tokens.append(expr[i:j])\n            i = j\n    return tokens\n\ndef collect_labels(tree):\n    \"\"\"\n    Duy·ªát c√¢y c·∫•u tr√∫c (nested list) ƒë·ªÉ thu th·∫≠p c√°c nh√£n c·ªßa quan h·ªá v√† th·ª±c th·ªÉ.\n    Gi·∫£ s·ª≠:\n      - Bi·ªÉu th·ª©c JOIN c√≥ d·∫°ng: [\"JOIN\", relation_part, entity_part]\n      - Ph·∫ßn relation_part: n·∫øu l√† list v√† b·∫Øt ƒë·∫ßu b·∫±ng \"R\", th√¨ ph·∫ßn th·ª© hai ch·ª©a nh√£n quan h·ªá (d·∫°ng \"[ label ]\"). \n        N·∫øu l√† chu·ªói d·∫°ng \"[ label ]\" th√¨ ƒë√≥ c≈©ng l√† nh√£n quan h·ªá.\n      - Ph·∫ßn entity_part: n·∫øu l√† chu·ªói d·∫°ng \"[ label ]\" th√¨ ƒë√≥ l√† nh√£n th·ª±c th·ªÉ, n·∫øu l√† list th√¨ x·ª≠ l√Ω ƒë·ªá quy.\n      - Bi·ªÉu th·ª©c AND s·∫Ω c√≥ nhi·ªÅu bi·ªÉu th·ª©c con.\n    \"\"\"\n    relations = []\n    entities = []\n    \n    if isinstance(tree, list) and tree:\n        # N·∫øu token ƒë·∫ßu ti√™n l√† JOIN ho·∫∑c AND\n        op = tree[0]\n        if isinstance(op, str):\n            op_upper = op.upper()\n        else:\n            op_upper = \"\"\n        \n        if op_upper == \"JOIN\":\n            # X·ª≠ l√Ω ph·∫ßn quan h·ªá\n            if len(tree) >= 2:\n                rel_part = tree[1]\n                # N·∫øu l√† list d·∫°ng [ \"R\", \"[ label ]\" ]\n                if isinstance(rel_part, list) and len(rel_part) >= 2 and isinstance(rel_part[0], str) and rel_part[0].upper() == \"R\":\n                    token = rel_part[1]\n                    if isinstance(token, str) and token.startswith('[') and token.endswith(']'):\n                        rel_label = token[1:-1].strip()\n                        relations.append(rel_label)\n                # N·∫øu l√† chu·ªói d·∫°ng \"[ label ]\"\n                elif isinstance(rel_part, str) and rel_part.startswith('[') and rel_part.endswith(']'):\n                    rel_label = rel_part[1:-1].strip()\n                    relations.append(rel_label)\n                else:\n                    # N·∫øu kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng, duy·ªát ƒë·ªá quy\n                    sub_rel, sub_ent = collect_labels(rel_part)\n                    relations.extend(sub_rel)\n                    entities.extend(sub_ent)\n            # X·ª≠ l√Ω ph·∫ßn th·ª±c th·ªÉ\n            if len(tree) >= 3:\n                ent_part = tree[2]\n                if isinstance(ent_part, list):\n                    sub_rel, sub_ent = collect_labels(ent_part)\n                    relations.extend(sub_rel)\n                    entities.extend(sub_ent)\n                elif isinstance(ent_part, str) and ent_part.startswith('[') and ent_part.endswith(']'):\n                    ent_label = ent_part[1:-1].strip()\n                    entities.append(ent_label)\n        elif op_upper == \"AND\":\n            # V·ªõi AND, duy·ªát t·∫•t c·∫£ c√°c ph·∫ßn con\n            for sub in tree[1:]:\n                sub_rel, sub_ent = collect_labels(sub)\n                relations.extend(sub_rel)\n                entities.extend(sub_ent)\n        else:\n            # N·∫øu kh√¥ng ph·∫£i JOIN hay AND, duy·ªát t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ n·∫øu ch√∫ng l√† list\n            for elem in tree:\n                if isinstance(elem, list):\n                    sub_rel, sub_ent = collect_labels(elem)\n                    relations.extend(sub_rel)\n                    entities.extend(sub_ent)\n    return relations, entities\n\n\n\ndef extract_entities_and_relations(normed_expr):\n\n    if not normed_expr or len(normed_expr) == 0:  # Ki·ªÉm tra n·∫øu normed_expr r·ªóng\n        return [], []\n    \n    if normed_expr[0] != \"(\":\n        return [], []\n    \n    tree = parse_nsexpr(normed_expr)\n    if tree is None:\n        return [], []  # Tr·∫£ v·ªÅ danh s√°ch r·ªóng n·∫øu parse th·∫•t b·∫°i\n    \n    return collect_labels(tree)\n\n\nclass SExpressionParser:\n    def __init__(self):\n        self.var_counter = 1  # ƒê·∫øm s·ªë bi·∫øn trung gian (?X1, ?X2, ...)\n\n    def get_new_var(self):\n        \"\"\"T·∫°o bi·∫øn trung gian m·ªõi.\"\"\"\n        var_name = f\"?X{self.var_counter}\"\n        self.var_counter += 1\n        return var_name\n\n    def parse_s_expr(self, s_expr):\n        \"\"\"Chuy·ªÉn ƒë·ªïi S-Expression th√†nh danh s√°ch l·ªìng nhau.\"\"\"\n        s_expr = re.sub(r'\\(', ' ( ', s_expr)\n        s_expr = re.sub(r'\\)', ' ) ', s_expr)\n        tokens = s_expr.split()\n        return self.build_tree(tokens)\n\n    def build_tree(self, tokens):\n        \"\"\"Chuy·ªÉn ƒë·ªïi danh s√°ch token th√†nh c√¢y l·ªìng nhau.\"\"\"\n        if not tokens:\n            return None\n        token = tokens.pop(0)\n        if token == \"(\":\n            sub_expr = []\n            while tokens[0] != \")\":\n                sub_expr.append(self.build_tree(tokens))\n            tokens.pop(0)  # B·ªè d·∫•u \")\"\n            return sub_expr\n        elif token == \")\":\n            raise ValueError(\"Unexpected ')'\")\n        else:\n            return token\n\n    def process_join(self, expr, target_var):\n        \"\"\"\n        X·ª≠ l√Ω JOIN, t·∫°o triple SPARQL.\n        \"\"\"\n        triples = []\n        if not isinstance(expr, list):\n            return expr, triples\n\n        if expr[0] == \"AND\":\n            # X·ª≠ l√Ω t·ª´ng JOIN trong AND ri√™ng l·∫ª\n            for sub_expr in expr[1:]:\n                _, sub_triples = self.process_join(sub_expr, target_var)\n                triples.extend(sub_triples)\n            return target_var, triples\n\n        if expr[0] == \"JOIN\":\n            right_expr = expr[2]\n            right_triples = []\n            if isinstance(right_expr, list) and right_expr[0] == \"JOIN\":\n                right_var, right_triples = self.process_join(right_expr, self.get_new_var())\n            else: \n                right_var = right_expr\n            \n            # X·ª≠ l√Ω nh√°nh tr√°i\n            left_expr = expr[1]\n            if isinstance(left_expr, list) and left_expr[0] == \"R\":\n                rel = left_expr[1]\n                right = right_var\n                left = target_var\n                if right[0] != '?':\n                    right = \"wd:\" + right\n                if left[0] !='?':\n                    left = \"wd:\" + left   \n                triples.append([right, f\"wdt:{rel}\", left])\n            else:\n                right = right_var\n                left = target_var\n                if right[0] != '?':\n                    right = \"wd:\" + right\n                if left[0] !='?':\n                    left = \"wd:\" + left   \n                triples.append([left, f\"wdt:{left_expr}\", right])\n\n            # Th√™m c√°c triples t·ª´ nh√°nh ph·∫£i tr∆∞·ªõc khi th√™m triple ch√≠nh\n            triples = right_triples + triples\n            return target_var, triples\n\n        return expr, triples\n\n    def s_expr_to_sparql(self, s_expr):\n        \"\"\"Chuy·ªÉn ƒë·ªïi t·ª´ S-Expression sang SPARQL.\"\"\"\n        if s_expr.count(\"(\") != s_expr.count(\")\"):\n            return None\n        if s_expr.count(\"[\") != s_expr.count(\"]\"):\n            return None\n        parsed_expr = self.parse_s_expr(s_expr)\n        target_var = \"?answer\"\n        final_var, triples = self.process_join(parsed_expr, target_var)\n\n        sparql_body = \"\\n  \".join([\" \".join(t) + \" .\" for t in triples])\n        sparql_query = f\"\"\"PREFIX wd: <http://www.wikidata.org/entity/> \nPREFIX wdt: <http://www.wikidata.org/prop/direct/> \nSELECT DISTINCT {target_var} WHERE {{ \n  {sparql_body}\n}}\"\"\"\n        return sparql_query\n\nWIKIDATA_SPARQL_ENDPOINT = \"https://query.wikidata.org/sparql\"\nWIKIDATA_API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n\ndef execute_query_with_odbc(sparql_query):\n    \"\"\"Truy v·∫•n Wikidata v√† tr·∫£ v·ªÅ danh s√°ch c√¢u tr·∫£ l·ªùi (bao g·ªìm t·∫•t c·∫£ bi·∫øn)\"\"\"\n    headers = {\"User-Agent\": \"Mozilla/5.0\", \"Accept\": \"application/sparql-results+json\"}\n    response = requests.get(WIKIDATA_SPARQL_ENDPOINT, params={\"query\": sparql_query, \"format\": \"json\"}, headers=headers)\n\n    if response.status_code == 200:\n        results = response.json().get(\"results\", {}).get(\"bindings\", [])\n        answers = []\n\n        for result in results:\n            for var in result:  # Duy·ªát qua t·∫•t c·∫£ c√°c bi·∫øn tr·∫£ v·ªÅ\n                value = result[var][\"value\"]\n                answers.append(value)  # Ch·∫•p nh·∫≠n t·∫•t c·∫£ gi√° tr·ªã, kh√¥ng ch·ªâ th·ª±c th·ªÉ Wikidata\n\n        return answers  # Tr·∫£ v·ªÅ to√†n b·ªô danh s√°ch k·∫øt qu·∫£\n\n    return []\n\ndef convert_normed_to_s_expression(normed_expr): #, gold_entity_map, gold_relation_map\n    \"\"\"\n    Chuy·ªÉn ƒë·ªïi t·ª´ normed_sexpression sang s_expression.\n    Sau khi tr√≠ch xu·∫•t c√°c nh√£n quan h·ªá v√† th·ª±c th·ªÉ t·ª´ normed_expr,\n    ta l·∫•y danh s√°ch c√°c m√£ ·ª©ng vi√™n cho m·ªói nh√£n v√† t·∫°o ho√°n v·ªã gi·ªØa c√°c c·∫∑p ·ª©ng vi√™n ƒë√≥.\n    K·∫øt qu·∫£ tr·∫£ v·ªÅ l√† m·ªôt danh s√°ch c√°c s_expression kh·∫£ dƒ©.\n    \"\"\"\n    normed_expr = fix_unbalanced_parentheses(normed_expr)\n    # Tr√≠ch xu·∫•t c√°c nh√£n quan h·ªá v√† th·ª±c th·ªÉ t·ª´ bi·ªÉu th·ª©c\n    relations, entities = extract_entities_and_relations(normed_expr)\n    # T·∫°o mapping t·ª´ token xu·∫•t hi·ªán trong bi·ªÉu th·ª©c sang danh s√°ch c√°c ·ª©ng vi√™n m√£.\n    # V√≠ d·ª•: token_str = \"[ author ]\"\n    s_expressions = []\n    candidate_map = {}\n    for rel in relations:\n       \n        token = f'[ {rel} ]'\n        candidate = search_wikidata_relations(str(rel))\n\n        # if not set(candidate)&set(gold_relation_map):\n        #     return s_expressions \n        # N·∫øu candidate kh√¥ng ph·∫£i danh s√°ch, chuy·ªÉn n√≥ th√†nh danh s√°ch ƒë·ªÉ t·∫°o ho√°n v·ªã\n        if not isinstance(candidate, list):\n            candidate = [candidate]\n        candidate_map[token] = candidate\n       \n    for ent in entities:\n        token = f'[ {ent} ]'\n        candidate = search_wikidata_entities(str(ent))\n        # if not set(candidate)&set(gold_entity_map):\n        #     return s_expressions \n        if not isinstance(candidate, list):\n            candidate = [candidate]\n        candidate_map[token] = candidate\n    \n    # N·∫øu kh√¥ng c√≥ token n√†o c·∫ßn thay th·∫ø, tr·∫£ v·ªÅ bi·ªÉu th·ª©c g·ªëc\n\n    if not candidate_map:\n        return [normed_expr]\n    \n    # L·∫•y danh s√°ch c√°c token v√† danh s√°ch c√°c danh s√°ch ·ª©ng vi√™n t∆∞∆°ng ·ª©ng\n    tokens = list(candidate_map.keys())\n    candidate_lists = [candidate_map[token] for token in tokens]\n    \n    # T·∫°o t·∫•t c·∫£ c√°c ho√°n v·ªã ·ª©ng vi√™n (Cartesian product)\n    all_combinations = list(product(*candidate_lists))\n    \n    for comb in all_combinations:\n        temp_expr = normed_expr\n        # V·ªõi m·ªói token, thay th·∫ø b·∫±ng ·ª©ng vi√™n t∆∞∆°ng ·ª©ng theo ho√°n v·ªã\n        for token, replacement in zip(tokens, comb):\n            temp_expr = temp_expr.replace(token, replacement)\n        s_expressions.append(temp_expr)\n    \n    return s_expressions\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:53:16.101829Z","iopub.execute_input":"2025-03-05T07:53:16.102092Z","iopub.status.idle":"2025-03-05T07:53:16.146681Z","shell.execute_reply.started":"2025-03-05T07:53:16.102058Z","shell.execute_reply":"2025-03-05T07:53:16.145746Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import time\n\ndef run_result(query):\n   \n    query_result = []\n    set_query = convert_normed_to_s_expression(query)\n\n    for q in set_query:\n        sparql = convert_s_expression_to_sparql(q)\n        if sparql == \"UNKNOWN\":\n            continue\n        \n        result = execute_query_with_odbc(sparql)\n\n        # N·∫øu k·∫øt qu·∫£ kh√¥ng r·ªóng, th√™m v√†o danh s√°ch\n        if result:\n            query_result.append(result)\n  \n    return query_result  # Tr·∫£ v·ªÅ danh s√°ch ch·ª©a danh s√°ch con (ch∆∞a x·ª≠ l√Ω)\n\n# X·ª≠ l√Ω nhi·ªÅu truy v·∫•n\ndef main(question):\n    start_time = time.time()\n    \n    queries = generate_query(question)\n    all_results = []  # Ch·ª©a t·∫•t c·∫£ k·∫øt qu·∫£ t·ª´ c√°c truy v·∫•n\n\n    for i, query in enumerate(queries, 1):\n        print(f\"üîπ Query {i}: {query}\")\n        query_result = run_result(query)\n        all_results.extend(query_result)  # Th√™m danh s√°ch k·∫øt qu·∫£ v√†o all_results\n\n    # G·ªôp danh s√°ch con v√† lo·∫°i b·ªè r·ªóng\n    final_results = sum(all_results, [])\n    end_time = time.time()\n    print(f\"Execution time: {end_time - start_time} seconds\")\n    print(\"üìå Final Results:\", final_results)  # K·∫øt qu·∫£ cu·ªëi c√πng\n\n# Ch·∫°y th·ª≠\nquestion = \"Vi·ªát Nam ·ªü ch√¢u l·ª•c n√†o?\"\nmain(question)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T08:11:06.712501Z","iopub.execute_input":"2025-03-05T08:11:06.712835Z","iopub.status.idle":"2025-03-05T08:11:14.569334Z","shell.execute_reply.started":"2025-03-05T08:11:06.712807Z","shell.execute_reply":"2025-03-05T08:11:14.568603Z"}},"outputs":[{"name":"stdout","text":"üîπ Query 1: ( JOIN [ l·ª•c ƒë·ªãa ] [ Vi·ªát Nam ] )\nüîπ Query 2: ( JOIN ( R [ n·∫±m trong m√∫i gi·ªù ] ) [ Vi·ªát Nam ] )\nüîπ Query 3: ( JOIN ( R [ n·∫±m trong ph·∫°m vi c·ªßa khu v·ª±c h√†nh ch√≠nh ] ) [ Vi·ªát Nam ] )\nüîπ Query 4: ( JOIN ( R [ l·ª•c ƒë·ªãa ] ) [ Vi·ªát Nam ] )\nüîπ Query 5: \nExecution time: 7.850265026092529 seconds\nüìå Final Results: ['http://www.wikidata.org/entity/Q63285961', 'http://www.wikidata.org/entity/Q6940', 'http://www.wikidata.org/entity/Q881', 'http://www.wikidata.org/entity/Q48', 'http://www.wikidata.org/entity/Q48', 'http://www.wikidata.org/entity/Q48']\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# # ƒê·ªãnh nghƒ©a d·ªØ li·ªáu ƒë·∫ßu v√†o cho API\n# class InputData(BaseModel):\n#     input_text: str\n\n# class QueryInput(BaseModel):\n#     question: str\n    \n# # T·∫°o endpoint ƒë·ªÉ g·ªçi model\n# @app.post(\"/predict\")\n# def predict(data: InputData):\n#     print(\"Received data:\", data)\n#     try:\n#         # Tokenize input\n#         input_ids = tokenizer.encode(data.input_text, return_tensors='pt').to(model.device)\n#         print(input_ids)\n#         # Generate output t·ª´ model\n#         output = model.generate(input_ids, max_length=100)\n#         output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n        \n#         print(\"Generated output:\", output_text)\n#         return {\"output_text\": output_text}\n#     except Exception as e:\n#         print(\"Error during prediction:\", str(e))\n#         return {\"error\": str(e)}\n        \n\n\n# # API generate_query\n# @app.post(\"/generate_query\")\n# def generate_query_api(data: QueryInput):\n#     try:\n#         print(\"Ques: \"+data.question)\n#         queries = generate_query(data.question)\n#         print (\"toiday\")\n       \n#         return {\"queries\": queries}\n#     except Exception as e:\n#         return {\"error\": str(e)}\n\n# # S·ª≠ d·ª•ng nest_asyncio ƒë·ªÉ tr√°nh l·ªói asyncio tr√™n Kaggle\n# nest_asyncio.apply()\n\n# # Th√™m authtoken c·ªßa b·∫°n t·∫°i ƒë√¢y\n# ngrok.set_auth_token(\"2tT6aa8W42iNmqXWfcwwAyzzokp_5Ko18vsKiFXBY6mz6sCwP\")\n\n# # M·ªü ngrok tunnel\n# public_url = ngrok.connect(8000)\n# print(f\"Public URL: {public_url}\")\n\n# # Ch·∫°y ·ª©ng d·ª•ng b·∫±ng Uvicorn\n# uvicorn.run(app, host='0.0.0.0', port=8000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:53:17.580867Z","iopub.execute_input":"2025-03-05T07:53:17.581216Z","iopub.status.idle":"2025-03-05T07:53:17.585097Z","shell.execute_reply.started":"2025-03-05T07:53:17.581172Z","shell.execute_reply":"2025-03-05T07:53:17.584153Z"}},"outputs":[],"execution_count":10}]}