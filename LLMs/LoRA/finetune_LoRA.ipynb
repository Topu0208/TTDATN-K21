{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":10724114,"datasetId":6648045,"databundleVersionId":11073891},{"sourceType":"datasetVersion","sourceId":10733334,"datasetId":6654879,"databundleVersionId":11084166},{"sourceType":"datasetVersion","sourceId":10784476,"datasetId":6692233,"databundleVersionId":11141096}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # K·∫øt qu·∫£: True (n·∫øu GPU ƒë√£ b·∫≠t)\nprint(torch.cuda.get_device_name(0))  # Ki·ªÉm tra lo·∫°i GPU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:29:53.495784Z","iopub.execute_input":"2025-02-18T14:29:53.496064Z","iopub.status.idle":"2025-02-18T14:29:57.341777Z","shell.execute_reply.started":"2025-02-18T14:29:53.496035Z","shell.execute_reply":"2025-02-18T14:29:57.341050Z"}},"outputs":[{"name":"stdout","text":"True\nTesla T4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import load_dataset\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:29:57.342540Z","iopub.execute_input":"2025-02-18T14:29:57.342846Z","iopub.status.idle":"2025-02-18T14:30:17.583367Z","shell.execute_reply.started":"2025-02-18T14:29:57.342826Z","shell.execute_reply":"2025-02-18T14:30:17.582460Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16,  # Rank c·ªßa LoRA, tƒÉng gi√° tr·ªã n√†y gi√∫p h·ªçc nhi·ªÅu h∆°n nh∆∞ng t·ªën VRAM\n    lora_alpha=32,  # H·ªá s·ªë ƒëi·ªÅu ch·ªânh m·ª©c ƒë·ªô h·ªçc c·ªßa LoRA\n    lora_dropout=0.05,  # Dropout gi√∫p tr√°nh overfitting\n    bias=\"none\",  # Kh√¥ng c·∫≠p nh·∫≠t bias ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n\n    task_type=TaskType.CAUSAL_LM,  # Lo·∫°i t√°c v·ª•\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:30:17.584192Z","iopub.execute_input":"2025-02-18T14:30:17.584775Z","iopub.status.idle":"2025-02-18T14:30:17.588692Z","shell.execute_reply.started":"2025-02-18T14:30:17.584749Z","shell.execute_reply":"2025-02-18T14:30:17.587816Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"model_path = \"vilm/vinallama-7b\"\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.float16)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\"M√¥ h√¨nh ƒë√£ t·∫£i th√†nh c√¥ng v·ªõi LoRA!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:41:35.275775Z","iopub.execute_input":"2025-02-18T14:41:35.276094Z","iopub.status.idle":"2025-02-18T14:41:42.273289Z","shell.execute_reply.started":"2025-02-18T14:41:35.276063Z","shell.execute_reply":"2025-02-18T14:41:42.272296Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ced0d9089b1468db0a9c82fad67a331"}},"metadata":{}},{"name":"stdout","text":"M√¥ h√¨nh ƒë√£ t·∫£i th√†nh c√¥ng v·ªõi LoRA!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from peft import get_peft_model\n\nfor param in model.parameters():\n    param.requires_grad = True\n\nmodel = get_peft_model(model, lora_config)  # K√≠ch ho·∫°t LoRA\nmodel.train()\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:41:42.274651Z","iopub.execute_input":"2025-02-18T14:41:42.274990Z","iopub.status.idle":"2025-02-18T14:41:42.467567Z","shell.execute_reply.started":"2025-02-18T14:41:42.274957Z","shell.execute_reply":"2025-02-18T14:41:42.466484Z"}},"outputs":[{"name":"stdout","text":"trainable params: 8,388,608 || all params: 6,863,974,400 || trainable%: 0.1222\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import json\nfrom datasets import Dataset\n\n# Load dataset t·ª´ Kaggle input\ndata_path = \"/kaggle/input/train-data/examples.json\"\n\nwith open(data_path, \"r\", encoding=\"utf-8\") as f:\n    dataset = json.load(f)\n\n# Ki·ªÉm tra d·ªØ li·ªáu ƒë·∫ßu v√†o\nassert isinstance(dataset, list), \"Dataset JSON ph·∫£i l√† danh s√°ch c√°c m·ª•c\"\n\n# Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang d·∫°ng vƒÉn b·∫£n hu·∫•n luy·ªán\ntrain_texts = [\n    f\"Instruction:\\n{item['instruction']}\\n\\nInput:\\n{item['input']}\\n\\nOutput:\\n{item['output']}\"\n    for item in dataset if \"instruction\" in item and \"input\" in item and \"output\" in item\n]\n\n# T·∫°o dataset t·ª´ Hugging Face Datasets\ntrain_dataset = Dataset.from_dict({\"text\": train_texts})\n\n# C·∫•u h√¨nh tokenizer\ntokenizer.pad_token = tokenizer.eos_token\n\n# Tokenize d·ªØ li·ªáu\ndef tokenize_function(example):\n    tokenized = tokenizer(\n        example[\"text\"],  \n        padding=\"max_length\",  # C√≥ th·ªÉ ƒë·ªïi th√†nh \"longest\" n·∫øu mu·ªën linh ho·∫°t h∆°n\n        truncation=True,\n        max_length=512,\n        return_tensors=\"pt\",\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # ‚úÖ Th√™m labels\n    return tokenized\n\n# Tokenize dataset\ntokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n\n# Ki·ªÉm tra k·∫øt qu·∫£\nprint(tokenized_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T14:41:42.469288Z","iopub.execute_input":"2025-02-18T14:41:42.469495Z","iopub.status.idle":"2025-02-18T14:41:44.538673Z","shell.execute_reply.started":"2025-02-18T14:41:42.469477Z","shell.execute_reply":"2025-02-18T14:41:44.537982Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba695e88fe3b436ba87b90fd6db62afd"}},"metadata":{}},{"name":"stdout","text":"{'input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2799, 4080, 29901, 13, 29911, 32168, 33002, 32557, 4522, 936, 3812, 32073, 32665, 32219, 27773, 32828, 32568, 32034, 32889, 32723, 32030, 3060, 29889, 29871, 13, 13, 13, 4290, 29901, 13, 16492, 29901, 426, 1281, 32009, 32641, 22392, 7712, 2681, 306, 18916, 7468, 29973, 500, 13, 13, 6466, 29901, 13, 29898, 8780, 313, 390, 518, 10329, 4514, 1723, 313, 8780, 313, 390, 518, 32040, 32797, 36396, 4514, 1723, 518, 22392, 7712, 2681, 306, 4514, 1723, 1723], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2799, 4080, 29901, 13, 29911, 32168, 33002, 32557, 4522, 936, 3812, 32073, 32665, 32219, 27773, 32828, 32568, 32034, 32889, 32723, 32030, 3060, 29889, 29871, 13, 13, 13, 4290, 29901, 13, 16492, 29901, 426, 1281, 32009, 32641, 22392, 7712, 2681, 306, 18916, 7468, 29973, 500, 13, 13, 6466, 29901, 13, 29898, 8780, 313, 390, 518, 10329, 4514, 1723, 313, 8780, 313, 390, 518, 32040, 32797, 36396, 4514, 1723, 518, 22392, 7712, 2681, 306, 4514, 1723, 1723]}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nfrom transformers import DataCollatorWithPadding\n\nMICRO_BATCH_SIZE = 1  # ‚úÖ Gi·∫£m xu·ªëng 1 ƒë·ªÉ ti·∫øt ki·ªám VRAM\nGRADIENT_ACCUMULATION_STEPS = 16  # ‚úÖ T·ªïng batch size = 1 * 16 = 16\nTRAIN_STEPS = 600  # ‚úÖ Train l√¢u h∆°n\nLEARNING_RATE = 1e-4  # ‚úÖ Gi·∫£m ƒë·ªÉ tr√°nh overfitting\n\n# Kh·ªüi t·∫°o collator v·ªõi tokenizer ƒë√£ load\ndata_collator = DataCollatorWithPadding(\n    tokenizer=tokenizer, \n    padding=True\n)\n\n# C·∫•u h√¨nh TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"./output\",\n    per_device_train_batch_size=MICRO_BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    warmup_ratio=0.03,\n    max_steps=TRAIN_STEPS,\n    learning_rate=LEARNING_RATE,\n    logging_steps=50,\n    optim='adamw_torch',\n    save_strategy=\"steps\",\n    save_steps=500,  # ‚úÖ Gi·∫£m s·ªë l·∫ßn l∆∞u checkpoint ƒë·ªÉ tr√°nh l√£ng ph√≠ b·ªô nh·ªõ\n    report_to=\"tensorboard\",\n    save_total_limit=1,\n    fp16=True,  # ‚úÖ B·∫≠t FP16 ƒë·ªÉ ti·∫øt ki·ªám VRAM\n    remove_unused_columns=False,  # ‚úÖ Fix l·ªói Trainer kh√¥ng nh·∫≠n d·ªØ li·ªáu\n    gradient_checkpointing=False,  # ‚úÖ Gi·∫£m b·ªô nh·ªõ ti√™u th·ª• khi hu·∫•n luy·ªán\n)\n\n# Kh·ªüi t·∫°o Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,  # ‚úÖ ƒê·∫£m b·∫£o padding ƒë√∫ng\n)\n\n# B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:04:47.164842Z","iopub.execute_input":"2025-02-18T15:04:47.165220Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='270' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [270/600 1:09:58 < 1:26:09, 0.06 it/s, Epoch 0.65/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>2.311100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.188500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.131700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.024300</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.977900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"def generate_query(question, history=[]):\n    input_text = f\"Instruction:\\nT·∫°o truy v·∫•n d·∫°ng logic ƒë·ªÉ l·∫•y th√¥ng tin t∆∞∆°ng ·ª©ng v·ªõi c√¢u h·ªèi ƒë√£ cho.\\n\\n Input:\\nQuestion: {question}\\n\\n Output:\\n\"\n    \n    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\n    output_ids = model.generate(\n        input_ids, \n        max_length=256,\n        temperature=0.2,\n        top_p=0.9,\n        do_sample=True,\n        eos_token_id=tokenizer.eos_token_id  # ‚úÖ D·ª´ng khi g·∫∑p eos_token\n    )\n    \n    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n    # C·∫Øt b·ªè ph·∫ßn d∆∞\n    output_text = output_text.split(\"Output:\")[-1].strip()\n    output_text = output_text.split(\"Input:\")[0].strip()  # ‚úÖ X√≥a ph·∫ßn d∆∞ th·ª´a\n\n    return output_text\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query1 = generate_query(\"Ai l√† ng∆∞·ªùi s√°ng l·∫≠p ra Open AI?\")\nquery2 = generate_query(\"Con c·ªßa ch·ªìng Ranavalona I l√† ai?\")\nquery3 = generate_query(\"Gi√°o vi√™n h∆∞·ªõng d·∫´n Shigeno Yasutsugu l√† ai?\")\n\nprint(\"üìå Truy v·∫•n 1:\", query1)\nprint(\"üìå Truy v·∫•n 2:\", query2)\nprint(\"üìå Truy v·∫•n 3:\", query3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\noutput_dir = \"/kaggle/working/model\"\nos.makedirs(output_dir, exist_ok=True)\n\n# L∆∞u m√¥ h√¨nh\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!tar -czvf vinallama7b_model.tar.gz -C /kaggle/working model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!!kaggle kernels output topunguyen/chatkbqa-vi -p /kaggle/working/\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}