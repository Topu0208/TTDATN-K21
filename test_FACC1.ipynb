{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trong\\anaconda3\\envs\\simcse_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from simcse import SimCSE\n",
    "from entity_retrieval import surface_index_memory\n",
    "# from your_module import convert_normed_to_s_expression  # Đổi 'your_module' thành tên file của bạn\n",
    "from simcse import SimCSE\n",
    "from itertools import product\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from tqdm import tqdm\n",
    "import ujson\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/23/2025 19:12:00 - INFO - simcse.tool -   Use `cls_before_pooler` for unsupervised models. If you want to use other pooling policy, specify `pooler` argument.\n",
      "02/23/2025 19:12:00 - INFO - entity_retrieval.surface_index_memory -   Building entity mid vocabulary.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vi_entity_list_file_wikidata_complete_all_mention'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m SimCSE(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprinceton-nlp/unsup-simcse-roberta-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m surface_index \u001b[38;5;241m=\u001b[39m \u001b[43msurface_index_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEntitySurfaceIndexMemory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvi_entity_list_file_wikidata_complete_all_mention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvi_surface_map_file_wikidata_all_mention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvi_surface_map_file_wikidata_complete_all_mention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\FINAL_TTDATN\\SOURCE CODE\\entity_retrieval\\surface_index_memory.py:27\u001b[0m, in \u001b[0;36mEntitySurfaceIndexMemory.__init__\u001b[1;34m(self, entity_list_file, surface_map_file, entity_index_prefix)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurface_map_file \u001b[38;5;241m=\u001b[39m surface_map_file\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# mid_vocabulary: {mid:offset}\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_entity_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity_index_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# surface_indxe: \u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurface_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_surface_index(entity_index_prefix)\n",
      "File \u001b[1;32md:\\FINAL_TTDATN\\SOURCE CODE\\entity_retrieval\\surface_index_memory.py:52\u001b[0m, in \u001b[0;36mEntitySurfaceIndexMemory._get_entity_vocabulary\u001b[1;34m(self, index_prefix)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    Mid vocabulary does not exist, build it.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_entity_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting entity vocabulary to disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m     marshal\u001b[38;5;241m.\u001b[39mdump(vocabulary, \u001b[38;5;28mopen\u001b[39m(vocab_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32md:\\FINAL_TTDATN\\SOURCE CODE\\entity_retrieval\\surface_index_memory.py:125\u001b[0m, in \u001b[0;36mEntitySurfaceIndexMemory._build_entity_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m num_lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Remember the offset for each entity.\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentity_list_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m# m=mmap.mmap(fileno, length[, flags[, prot[, access[, offset]]]])\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     mm \u001b[38;5;241m=\u001b[39m mmap\u001b[38;5;241m.\u001b[39mmmap(f\u001b[38;5;241m.\u001b[39mfileno(), \u001b[38;5;241m0\u001b[39m, access\u001b[38;5;241m=\u001b[39mmmap\u001b[38;5;241m.\u001b[39mACCESS_READ) \u001b[38;5;66;03m# create a mmap object\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     offset \u001b[38;5;241m=\u001b[39m mm\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;66;03m# return the pointer\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vi_entity_list_file_wikidata_complete_all_mention'"
     ]
    }
   ],
   "source": [
    "model = SimCSE(\"princeton-nlp/unsup-simcse-roberta-large\")\n",
    "\n",
    "surface_index = surface_index_memory.EntitySurfaceIndexMemory(\n",
    "    \"vi_entity_list_file_wikidata_complete_all_mention\", \"vi_surface_map_file_wikidata_all_mention\",\n",
    "    \"vi_surface_map_file_wikidata_complete_all_mention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_expression(expr):\n",
    "    \"\"\"Kiểm tra tính hợp lệ của biểu thức bằng cách đếm số ngoặc mở và đóng.\"\"\"\n",
    "    count = 0\n",
    "    for char in expr:\n",
    "        if char == '(':\n",
    "            count += 1\n",
    "        elif char == ')':\n",
    "            count -= 1\n",
    "        if count < 0:\n",
    "            return False  # Gặp ngoặc đóng trước ngoặc mở\n",
    "    return count == 0\n",
    "\n",
    "def fix_unbalanced_parentheses(expr):\n",
    "    \"\"\"Loại bỏ ngoặc đóng dư nếu có.\"\"\"\n",
    "    while not is_valid_expression(expr) and expr.endswith(')'):\n",
    "        expr = expr[:-1]\n",
    "    return expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_entity_or_relation(label, label_map, simcse_model, facc1_index, top_k=50, similarity_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Tìm mã tương ứng cho thực thể hoặc quan hệ từ gold maps, SimCSE hoặc FACC1.\n",
    "    Nếu label_map rỗng, sẽ bỏ qua bước similarity và chuyển thẳng sang tra cứu FACC1.\n",
    "    \"\"\"\n",
    "    search = True\n",
    "    label_lower = label.lower()\n",
    "    # Nếu label_map không rỗng, thử tra cứu qua label_map và similarity\n",
    "    if label_map:\n",
    "        if label_lower in label_map:\n",
    "            return label_map[label_lower]\n",
    "        similarities = simcse_model.similarity([label_lower], list(label_map.keys()))\n",
    "        if list(label_map.keys()):\n",
    "            merged_list = list(zip(label_map.keys(), similarities[0]))\n",
    "            sorted_list = sorted(merged_list, key=lambda x: x[1], reverse=True)\n",
    "            if sorted_list and sorted_list[0][1] > similarity_threshold:\n",
    "                return label_map[sorted_list[0][0]]\n",
    "    \n",
    "    # Nếu không có dữ liệu từ label_map, chuyển thẳng sang tra cứu FACC1\n",
    "    try:\n",
    "        print(label)\n",
    "        facc1_candidates = facc1_index.get_indexrange_entity_el_pro_one_mention(label, top_k=top_k)\n",
    "    except Exception as e:\n",
    "        # Nếu có lỗi xảy ra trong quá trình tra cứu FACC1, log lỗi và trả về label gốc\n",
    "        print(f\"Lỗi khi truy xuất FACC1 cho label '{label}': {e}\")\n",
    "        return label\n",
    "\n",
    "    if facc1_candidates:\n",
    "        keys = list(facc1_candidates.keys())\n",
    "        if not keys:\n",
    "            return label\n",
    "        try:\n",
    "            # Lấy candidate đầu tiên và các candidate có điểm >= 0.001\n",
    "            temp = [key for key in keys[1:] if facc1_candidates[key] >= 0.001]\n",
    "            return [keys[0]] + temp if temp else keys[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi xử lý kết quả FACC1 cho label '{label}': {e}\")\n",
    "            return label\n",
    "\n",
    "    return label\n",
    "\n",
    "# def invert_map(original_map):\n",
    "#     \"\"\"Đảo ngược key-value trong map để tìm mã từ tên.\"\"\"\n",
    "#     return {v.lower(): k for k, v in original_map.items()}\n",
    "\n",
    "def parse_nsexpr(expr):\n",
    "    \"\"\"\n",
    "    Chuyển chuỗi biểu thức thành cây cấu trúc dạng nested list.\n",
    "    Hàm này dùng duyệt ký tự, khi gặp '(' sẽ tìm phần con cho đến khi khớp với ')',\n",
    "    và giữ nguyên nội dung trong ngoặc vuông.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(expr):\n",
    "        if expr[i].isspace():\n",
    "            i += 1\n",
    "        elif expr[i] == '(':\n",
    "            # Tìm phần con của biểu thức trong ngoặc đơn\n",
    "            count = 1\n",
    "            j = i + 1\n",
    "            while j < len(expr) and count > 0:\n",
    "                if expr[j] == '(':\n",
    "                    count += 1\n",
    "                elif expr[j] == ')':\n",
    "                    count -= 1\n",
    "                j += 1\n",
    "            # Đệ quy phân tích phần con (loại bỏ ngoặc bao ngoài)\n",
    "            subtree = parse_nsexpr(expr[i+1:j-1])\n",
    "            tokens.append(subtree)\n",
    "            i = j\n",
    "        elif expr[i] == '[':\n",
    "            # Giữ nguyên nội dung trong ngoặc vuông\n",
    "            j = expr.find(']', i)\n",
    "            if j == -1:\n",
    "                return \"\"\n",
    "                raise ValueError(\"Không tìm thấy dấu ']' kết thúc.\")\n",
    "                \n",
    "            token = expr[i:j+1].strip()\n",
    "            tokens.append(token)\n",
    "            i = j + 1\n",
    "        else:\n",
    "            # Đọc một token cho đến khi gặp khoảng trắng hoặc ngoặc\n",
    "            j = i\n",
    "            while j < len(expr) and (not expr[j].isspace()) and expr[j] not in ['(', ')']:\n",
    "                j += 1\n",
    "            tokens.append(expr[i:j])\n",
    "            i = j\n",
    "    return tokens\n",
    "\n",
    "def collect_labels(tree):\n",
    "    \"\"\"\n",
    "    Duyệt cây cấu trúc (nested list) để thu thập các nhãn của quan hệ và thực thể.\n",
    "    Giả sử:\n",
    "      - Biểu thức JOIN có dạng: [\"JOIN\", relation_part, entity_part]\n",
    "      - Phần relation_part: nếu là list và bắt đầu bằng \"R\", thì phần thứ hai chứa nhãn quan hệ (dạng \"[ label ]\"). \n",
    "        Nếu là chuỗi dạng \"[ label ]\" thì đó cũng là nhãn quan hệ.\n",
    "      - Phần entity_part: nếu là chuỗi dạng \"[ label ]\" thì đó là nhãn thực thể, nếu là list thì xử lý đệ quy.\n",
    "      - Biểu thức AND sẽ có nhiều biểu thức con.\n",
    "    \"\"\"\n",
    "    relations = []\n",
    "    entities = []\n",
    "    \n",
    "    if isinstance(tree, list) and tree:\n",
    "        # Nếu token đầu tiên là JOIN hoặc AND\n",
    "        op = tree[0]\n",
    "        if isinstance(op, str):\n",
    "            op_upper = op.upper()\n",
    "        else:\n",
    "            op_upper = \"\"\n",
    "        \n",
    "        if op_upper == \"JOIN\":\n",
    "            # Xử lý phần quan hệ\n",
    "            if len(tree) >= 2:\n",
    "                rel_part = tree[1]\n",
    "                # Nếu là list dạng [ \"R\", \"[ label ]\" ]\n",
    "                if isinstance(rel_part, list) and len(rel_part) >= 2 and isinstance(rel_part[0], str) and rel_part[0].upper() == \"R\":\n",
    "                    token = rel_part[1]\n",
    "                    if isinstance(token, str) and token.startswith('[') and token.endswith(']'):\n",
    "                        rel_label = token[1:-1].strip()\n",
    "                        relations.append(rel_label)\n",
    "                # Nếu là chuỗi dạng \"[ label ]\"\n",
    "                elif isinstance(rel_part, str) and rel_part.startswith('[') and rel_part.endswith(']'):\n",
    "                    rel_label = rel_part[1:-1].strip()\n",
    "                    relations.append(rel_label)\n",
    "                else:\n",
    "                    # Nếu không đúng định dạng, duyệt đệ quy\n",
    "                    sub_rel, sub_ent = collect_labels(rel_part)\n",
    "                    relations.extend(sub_rel)\n",
    "                    entities.extend(sub_ent)\n",
    "            # Xử lý phần thực thể\n",
    "            if len(tree) >= 3:\n",
    "                ent_part = tree[2]\n",
    "                if isinstance(ent_part, list):\n",
    "                    sub_rel, sub_ent = collect_labels(ent_part)\n",
    "                    relations.extend(sub_rel)\n",
    "                    entities.extend(sub_ent)\n",
    "                elif isinstance(ent_part, str) and ent_part.startswith('[') and ent_part.endswith(']'):\n",
    "                    ent_label = ent_part[1:-1].strip()\n",
    "                    entities.append(ent_label)\n",
    "        elif op_upper == \"AND\":\n",
    "            # Với AND, duyệt tất cả các phần con\n",
    "            for sub in tree[1:]:\n",
    "                sub_rel, sub_ent = collect_labels(sub)\n",
    "                relations.extend(sub_rel)\n",
    "                entities.extend(sub_ent)\n",
    "        else:\n",
    "            # Nếu không phải JOIN hay AND, duyệt tất cả các phần tử nếu chúng là list\n",
    "            for elem in tree:\n",
    "                if isinstance(elem, list):\n",
    "                    sub_rel, sub_ent = collect_labels(elem)\n",
    "                    relations.extend(sub_rel)\n",
    "                    entities.extend(sub_ent)\n",
    "    return relations, entities\n",
    "\n",
    "\n",
    "\n",
    "def extract_entities_and_relations(normed_expr):\n",
    "\n",
    "    if not normed_expr or len(normed_expr) == 0:  # Kiểm tra nếu normed_expr rỗng\n",
    "        return [], []\n",
    "    \n",
    "    if normed_expr[0] != \"(\":\n",
    "        return [], []\n",
    "    \n",
    "    tree = parse_nsexpr(normed_expr)\n",
    "    if tree is None:\n",
    "        return [], []  # Trả về danh sách rỗng nếu parse thất bại\n",
    "    \n",
    "    return collect_labels(tree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_normed_to_s_expression(normed_expr, gold_relation_map, gold_entity_map, simcse_model, facc1_index):\n",
    "    \"\"\"\n",
    "    Chuyển đổi từ normed_sexpression sang s_expression.\n",
    "    Sau khi trích xuất các nhãn quan hệ và thực thể từ normed_expr,\n",
    "    ta lấy danh sách các mã ứng viên cho mỗi nhãn và tạo hoán vị giữa các cặp ứng viên đó.\n",
    "    Kết quả trả về là một danh sách các s_expression khả dĩ.\n",
    "    \"\"\"\n",
    "    # Đảo ngược map để lấy mã từ tên\n",
    "    # inverted_relation_map = invert_map(gold_relation_map)\n",
    "    # inverted_entity_map = invert_map(gold_entity_map)\n",
    "    normed_expr = fix_unbalanced_parentheses(normed_expr)\n",
    "    # Trích xuất các nhãn quan hệ và thực thể từ biểu thức\n",
    "    relations, entities = extract_entities_and_relations(normed_expr)\n",
    "    \n",
    "    # Tạo mapping từ token xuất hiện trong biểu thức sang danh sách các ứng viên mã.\n",
    "    # Ví dụ: token_str = \"[ author ]\"\n",
    "    candidate_map = {}\n",
    "    \n",
    "    for rel in relations:\n",
    "        token = f'[ {rel} ]'\n",
    "        candidate = find_entity_or_relation(rel, gold_relation_map, simcse_model, facc1_index)\n",
    "        # Nếu candidate không phải danh sách, chuyển nó thành danh sách để tạo hoán vị\n",
    "        if not isinstance(candidate, list):\n",
    "            candidate = [candidate]\n",
    "        candidate_map[token] = candidate\n",
    "        \n",
    "    for ent in entities:\n",
    "        token = f'[ {ent} ]'\n",
    "        candidate = find_entity_or_relation(ent, gold_entity_map, simcse_model, facc1_index)\n",
    "        if not isinstance(candidate, list):\n",
    "            candidate = [candidate]\n",
    "        candidate_map[token] = candidate\n",
    "    \n",
    "    # Nếu không có token nào cần thay thế, trả về biểu thức gốc\n",
    "    if not candidate_map:\n",
    "        return [normed_expr]\n",
    "    \n",
    "    # Lấy danh sách các token và danh sách các danh sách ứng viên tương ứng\n",
    "    tokens = list(candidate_map.keys())\n",
    "    candidate_lists = [candidate_map[token] for token in tokens]\n",
    "    \n",
    "    # Tạo tất cả các hoán vị ứng viên (Cartesian product)\n",
    "    all_combinations = list(product(*candidate_lists))\n",
    "    \n",
    "    s_expressions = []\n",
    "    for comb in all_combinations:\n",
    "        temp_expr = normed_expr\n",
    "        # Với mỗi token, thay thế bằng ứng viên tương ứng theo hoán vị\n",
    "        for token, replacement in zip(tokens, comb):\n",
    "            temp_expr = temp_expr.replace(token, replacement)\n",
    "        s_expressions.append(temp_expr)\n",
    "    \n",
    "    return s_expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SExpressionParser:\n",
    "    def __init__(self):\n",
    "        self.var_counter = 1  # Đếm số biến trung gian (?X1, ?X2, ...)\n",
    "\n",
    "    def get_new_var(self):\n",
    "        \"\"\"Tạo biến trung gian mới.\"\"\"\n",
    "        var_name = f\"?X{self.var_counter}\"\n",
    "        self.var_counter += 1\n",
    "        return var_name\n",
    "\n",
    "    def parse_s_expr(self, s_expr):\n",
    "        \"\"\"Chuyển đổi S-Expression thành danh sách lồng nhau.\"\"\"\n",
    "        s_expr = re.sub(r'\\(', ' ( ', s_expr)\n",
    "        s_expr = re.sub(r'\\)', ' ) ', s_expr)\n",
    "        tokens = s_expr.split()\n",
    "        return self.build_tree(tokens)\n",
    "\n",
    "    def build_tree(self, tokens):\n",
    "        \"\"\"Chuyển đổi danh sách token thành cây lồng nhau.\"\"\"\n",
    "        if not tokens:\n",
    "            return None\n",
    "        token = tokens.pop(0)\n",
    "        if token == \"(\":\n",
    "            sub_expr = []\n",
    "            while tokens[0] != \")\":\n",
    "                sub_expr.append(self.build_tree(tokens))\n",
    "            tokens.pop(0)  # Bỏ dấu \")\"\n",
    "            return sub_expr\n",
    "        elif token == \")\":\n",
    "            raise ValueError(\"Unexpected ')'\")\n",
    "        else:\n",
    "            return token\n",
    "\n",
    "    def process_join(self, expr, target_var):\n",
    "        \"\"\"\n",
    "        Xử lý JOIN, tạo triple SPARQL.\n",
    "        \"\"\"\n",
    "        triples = []\n",
    "        if not isinstance(expr, list):\n",
    "            return expr, triples\n",
    "\n",
    "        if expr[0] == \"AND\":\n",
    "            # Xử lý từng JOIN trong AND riêng lẻ\n",
    "            for sub_expr in expr[1:]:\n",
    "                _, sub_triples = self.process_join(sub_expr, target_var)\n",
    "                triples.extend(sub_triples)\n",
    "            return target_var, triples\n",
    "\n",
    "        if expr[0] == \"JOIN\":\n",
    "            right_expr = expr[2]\n",
    "            right_triples = []\n",
    "            if isinstance(right_expr, list) and right_expr[0] == \"JOIN\":\n",
    "                right_var, right_triples = self.process_join(right_expr, self.get_new_var())\n",
    "            else: \n",
    "                right_var = right_expr\n",
    "            \n",
    "            # Xử lý nhánh trái\n",
    "            left_expr = expr[1]\n",
    "            if isinstance(left_expr, list) and left_expr[0] == \"R\":\n",
    "                rel = left_expr[1]\n",
    "                right = right_var\n",
    "                left = target_var\n",
    "                if right[0] != '?':\n",
    "                    right = \"wd:\" + right\n",
    "                if left[0] !='?':\n",
    "                    left = \"wd:\" + left   \n",
    "                triples.append([right, f\"wdt:{rel}\", left])\n",
    "            else:\n",
    "                right = right_var\n",
    "                left = target_var\n",
    "                if right[0] != '?':\n",
    "                    right = \"wd:\" + right\n",
    "                if left[0] !='?':\n",
    "                    left = \"wd:\" + left   \n",
    "                triples.append([left, f\"wdt:{left_expr}\", right])\n",
    "\n",
    "            # Thêm các triples từ nhánh phải trước khi thêm triple chính\n",
    "            triples = right_triples + triples\n",
    "            return target_var, triples\n",
    "\n",
    "        return expr, triples\n",
    "\n",
    "    def s_expr_to_sparql(self, s_expr):\n",
    "        \"\"\"Chuyển đổi từ S-Expression sang SPARQL.\"\"\"\n",
    "        parsed_expr = self.parse_s_expr(s_expr)\n",
    "        target_var = \"?answer\"\n",
    "        final_var, triples = self.process_join(parsed_expr, target_var)\n",
    "\n",
    "        sparql_body = \"\\n  \".join([\" \".join(t) + \" .\" for t in triples])\n",
    "        sparql_query = f\"\"\"PREFIX wd: <http://www.wikidata.org/entity/> \n",
    "PREFIX wdt: <http://www.wikidata.org/prop/direct/> \n",
    "SELECT DISTINCT {target_var} WHERE {{ \n",
    "  {sparql_body}\n",
    "}}\"\"\"\n",
    "        return sparql_query\n",
    "\n",
    "\n",
    "def execute_query_with_odbc(sparql_query):\n",
    "    \"\"\"\n",
    "    Thực thi truy vấn SPARQL trên Wikidata endpoint và trả về kết quả.\n",
    "    \"\"\"\n",
    "    ENDPOINT_URL = \"https://query.wikidata.org/sparql\"\n",
    "    if sparql_query == None or sparql_query == []:\n",
    "        return []\n",
    "    sparql = SPARQLWrapper(ENDPOINT_URL)\n",
    "    sparql.setQuery(str(sparql_query))  # Ép kiểu về str cho chắc chắn\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    try:\n",
    "        response = sparql.query().convert()\n",
    "        answers = [item[\"answer\"][\"value\"] for item in response[\"results\"][\"bindings\"] if \"answer\" in item]\n",
    "        return answers\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Đọc file JSONL và trả về danh sách các object\"\"\"\n",
    "\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read().replace('(EXPECTED RESULT)', 'null').replace('(QUESTION)', 'null')\n",
    "\n",
    "    try:\n",
    "        data = ujson.loads(content)\n",
    "        print(\"JSON loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON loaded successfully!\n",
      "JSON loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "predictions = load_jsonl(\"LLMs/beam_prediction/generated_predictions_beam.json\")\n",
    "gold_data = load_jsonl(\"Data/LC-QuAD2.0/label_map/LC-QuAD2.0_test.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "simcse_model = model  # Mô hình SimCSE của bạn\n",
    "facc1_index = surface_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_entity_map = {}\n",
    "gold_relation_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"( AND ( JOIN [ occupation ] [ singer ] ) ( JOIN ( R [ voice actor ] ) [ South Park ] ) )\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occupation\n",
      "voice actor\n",
      "singer\n",
      "South Park\n",
      "Lỗi khi truy xuất FACC1 cho label 'South Park': list index out of range\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['( AND ( JOIN occupation singer ) ( JOIN ( R voice actor ) South Park ) )']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_query = convert_normed_to_s_expression(query, gold_relation_map, gold_entity_map, simcse_model, facc1_index)\n",
    "set_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
